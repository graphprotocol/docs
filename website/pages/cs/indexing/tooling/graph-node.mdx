---
title: Uzel provozního graf
---

Graf Uzel je komponenta, která indexuje podgrafy a zpřístupňuje výsledná data k dotazování prostřednictvím rozhraní GraphQL API. Jako taková je ústředním prvkem zásobníku indexeru a její správná činnost je pro úspěšný provoz indexeru klíčová.

Tato část poskytuje kontextový přehled o uzlu Graf a o některých pokročilejších možnostech, které jsou indexátorům k dispozici. Podrobnou dokumentaci a pokyny najdete v úložišti [Graf Uzel](https://github.com/graphprotocol/graph-node).

## Uzel Graf

[Graf Uzel](https://github.com/graphprotocol/graph-node) je referenční implementace pro indexování podgrafů v síti Graf, připojení ke klientům blockchainu, indexování podgrafů a zpřístupnění indexovaných dat k dotazování.

Graf Uzel (a celý indexer stack) lze provozovat na holém železe nebo v cloudovém prostředí. Tato flexibilita centrální indexovací komponenty je klíčová pro robustnost protokolu v Graf Protocol. Stejně tak může být Graf Uzel [postaven ze zdrojového kódu](https://github.com/graphprotocol/graph-node), nebo mohou indexátory používat jeden z [poskytovaných obrazů Docker](https://hub.docker.com/r/graphprotocol/graph-node).

### Databáze PostgreSQL

Hlavní úložiště pro uzel Graf Uzel, kde jsou uložena data podgrafů, metadata o podgraf a síťová data týkající se podgrafů, jako je bloková cache a cache eth_call.

### Síťoví klienti

Aby mohl uzel Graph Node indexovat síť, potřebuje přístup k síťovému klientovi prostřednictvím rozhraní API JSON-RPC kompatibilního s EVM. Toto RPC se může připojit k jedinému klientovi nebo může jít o složitější nastavení, které vyrovnává zátěž mezi více klienty.

Zatímco některé podgrafy mohou vyžadovat pouze plný uzel, některé mohou mít indexovací funkce, které vyžadují další funkce RPC. Konkrétně podgrafy, které v rámci indexování provádějí `eth_calls`, budou vyžadovat archivní uzel, který podporuje [EIP-1898](https://eips.ethereum.org/EIPS/eip-1898), a podgrafy s `callHandlers` nebo `blockHandlers` s filtrem `call` vyžadují podporu `trace_filter` ([viz dokumentace modulu trace zde](https://openethereum.github.io/JSONRPC-trace-module)).

**Network Firehoses** - Firehose je služba gRPC poskytující uspořádaný, ale vidlicově orientovaný proud bloků, vyvinutá hlavními vývojáři Grafu pro lepší podporu výkonného indexování v měřítku. V současné době to není požadavek na indexátor, ale doporučujeme indexátorům, aby se s touto technologií seznámili ještě před plnou podporou sítě. Více informací o Firehose [zde](https://firehose.streamingfast.io/).

### IPFS uzly

Metadata nasazení podgrafů jsou uložena v síti IPFS. Uzel Graf přistupuje během nasazení podgrafu především k uzlu IPFS, aby načetl manifest podgrafu a všechny propojené soubory. Síťové indexery nemusí hostit vlastní uzel IPFS. Uzel IPFS pro síť je hostován na adrese https://ipfs.network.thegraph.com.

### Metrický server Prometheus

Aby bylo možné monitorovat a podávat zprávy, může uzel Graf volitelně zaznamenávat metriky na metrický server Prometheus.

### Začínáme od zdroje

#### Instalace předpokladů

- **Rust**

- **PostgreSQL**

- **IPFS**

- **Další požadavky pro uživatele Ubuntu** - Pro spuštění Uzel Graf v Ubuntu může být potřeba několik dalších balíčků.

```sh
sudo apt-get install -y clang libpq-dev libssl-dev pkg-config
```

#### Nastavení

1. Spuštění databázového serveru PostgreSQL

```sh
initdb -D .postgres
pg_ctl -D .postgres -l logfile start
createdb graph-node
```

2. Klonujte repozitář [Uzel Graf](https://github.com/graphprotocol/graph-node) a sestavte zdrojový kód spuštěním příkazu `cargo build`

3. Nyní, když jsou všechny závislosti nastaveny, spusťte uzel Graf:

```sh
cargo run -p graph-node --release -- \
  --postgres-url postgresql://[USERNAME]:[PASSWORD]@localhost:5432/graph-node \
  --ethereum-rpc [NETWORK_NAME]:[URL] \
  --ipfs https://ipfs.network.thegraph.com
```

### Začínáme s Kubernetes

Kompletní příklad konfigurace Kubernetes naleznete v úložišti [indexer](https://github.com/graphprotocol/indexer/tree/main/k8s).

### Ports

Když je Graf Uzel spuštěn, zpřístupňuje následující ports:

| Port | Účel | Trasy | CLI Argument | Proměnná prostředí |
| --- | --- | --- | --- | --- |
| 8000 | GraphQL HTTP server<br />(pro dotazy podgrafy) | /subgraphs/id/...<br />/subgraphs/name/.../... | --http-port | - |
| 8001 | GraphQL WS<br />(pro odběry podgrafů) | /subgraphs/id/...<br />/subgraphs/name/.../... | --ws-port | - |
| 8020 | JSON-RPC<br />(pro správu nasazení) | / | --admin-port | - |
| 8030 | Stav indexování podgrafů API | /graphql | --index-node-port | - |
| 8040 | Metriky Prometheus | /metrics | --metrics-port | - |

> **Důležité**: Dávejte pozor na veřejné vystavování portů - **administrační porty** by měly být uzamčeny. To se týká i koncového bodu JSON-RPC uzlu Graf.

## Pokročilá konfigurace uzlu Graf

V nejjednodušším případě lze Graf Uzel provozovat s jednou instancí Graf Uzel, jednou databází PostgreSQL, uzlem IPFS a síťovými klienty podle potřeby indexovaných podgrafů.

Toto nastavení lze horizontálně škálovat přidáním více graf uzlů a více databází pro podporu těchto graf uzlů. Pokročilí uživatelé mohou chtít využít některé možnosti horizontálního škálování Graf Uzel a také některé pokročilejší možnosti konfigurace prostřednictvím souboru `config.toml` a proměnných prostředí Graph Node.

### `config.toml`

Konfigurační soubor [TOML](https://toml.io/en/) lze použít k nastavení složitějších konfigurací, než jaké jsou dostupné v CLI. Umístění souboru se předává pomocí přepínače příkazového řádku --config.

> Při použití konfiguračního souboru není možné použít volby --postgres-url, --postgres-secondary-hosts a --postgres-host-weights.

Lze zadat minimální soubor `config.toml`; následující soubor je ekvivalentní použití volby příkazového řádku --postgres-url:

```toml
[store]
[store.primary]
connection="<.. postgres-url argument ..>"
[deployment]
[[deployment.rule]]
indexers = [ "<.. list of all indexing nodes ..>" ]
```

Úplnou dokumentaci `config.toml` lze nalézt v [dokumentech Graf Uzel](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md).

#### Více uzlů graf

Graph Node indexing can scale horizontally, running multiple instances of Graph Node to split indexing and querying across different nodes. This can be done simply by running Graph Nodes configured with a different `node_id` on startup (e.g. in the Docker Compose file), which can then be used in the `config.toml` file to specify [dedicated query nodes](#dedicated-query-nodes), [block ingestors](#dedicated-block-ingestion), and splitting subgraphs across nodes with [deployment rules](#deployment-rules).

> Všimněte si, že více graf uzlů lze nakonfigurovat tak, aby používaly stejnou databázi, kterou lze horizontálně škálovat pomocí sharding.

#### Pravidla nasazení

Při více uzlech graf je nutné řídit nasazení nových podgrafů tak, aby stejný podgraf nebyl indexován dvěma různými uzly, což by vedlo ke kolizím. To lze provést pomocí pravidel nasazení, která mohou také určit, do kterého `shardu` mají být data podgrafu uložena, pokud se používá rozdělení databáze. Pravidla nasazení mohou odpovídat názvu podgrafu a síti, kterou nasazení indexuje, aby bylo možné učinit rozhodnutí.

Příklad konfigurace pravidla nasazení:

```toml
[deployment]
[[deployment.rule]]
match = { name = "(vip|important)/.*" }
shard = "vip"
indexers = [ "index_node_vip_0", "index_node_vip_1" ]
[[deployment.rule]]
match = { network = "kovan" }
# No shard, so we use the default shard called 'primary'
indexers = [ "index_node_kovan_0" ]
[[deployment.rule]]
match = { network = [ "xdai", "poa-core" ] }
indexers = [ "index_node_other_0" ]
[[deployment.rule]]
# There's no 'match', so any subgraph matches
shards = [ "sharda", "shardb" ]
indexers = [
    "index_node_community_0",
    "index_node_community_1",
    "index_node_community_2",
    "index_node_community_3",
    "index_node_community_4",
    "index_node_community_5"
  ]
```

Více informací o pravidlech nasazení [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#controlling-deployment).

#### Vyhrazené dotazovací uzly

Uzly lze nakonfigurovat tak, aby byly explicitně dotazovacími uzly, a to tak, že do konfiguračního souboru vložíte následující údaje:

```toml
[general]
query = "<regular expression>"
```

Každý uzel, jehož --node-id odpovídá regulárnímu výrazu, bude nastaven tak, aby odpovídal pouze na dotazy.

#### Škálování databáze pomocí sharding

Pro většinu případů použití postačuje k podpoře instance graf uzlu jedna databáze Postgres. Pokud instance graf uzlu přeroste rámec jedné databáze Postgres, je možné rozdělit ukládání dat grafového uzlu do více databází Postgres. Všechny databáze dohromady tvoří úložiště instance graf uzlu. Každá jednotlivá databáze se nazývá shard.

Střepy lze použít k rozdělení nasazení dílčích graf do více databází a lze je také použít k použití replik k rozložení zátěže dotazů mezi databázemi. To zahrnuje konfiguraci počtu dostupných databázových připojení, které by měl každý `graf-node` udržovat ve svém fondu připojení pro každou databázi, což je stále důležitější, když se indexuje více podgrafů.

Sharding se stává užitečným, když vaše stávající databáze nedokáže udržet krok se zátěží, kterou na ni Graf Uzel vyvíjí, a když už není možné zvětšit velikost databáze.

> Obecně je lepší vytvořit jednu co největší databázi, než začít s oddíly. Jednou z výjimek jsou případy, kdy je provoz dotazů rozdělen velmi nerovnoměrně mezi dílčí podgrafy; v těchto situacích může výrazně pomoci, pokud jsou dílčí podgrafy s velkým objemem uchovávány v jednom shardu a vše ostatní v jiném, protože toto nastavení zvyšuje pravděpodobnost, že data pro dílčí podgrafu s velkým objemem zůstanou v interní cache db a nebudou nahrazena daty, která nejsou tolik potřebná z dílčích podgrafů s malým objemem.

Pokud jde o konfiguraci připojení, začněte s max_connections v souboru postgresql.conf nastaveným na 400 (nebo možná dokonce 200) a podívejte se na metriky store_connection_wait_time_ms a store_connection_checkout_count Prometheus. Výrazné čekací doby (cokoli nad 5 ms) jsou známkou toho, že je k dispozici příliš málo připojení; vysoké čekací doby tam budou také způsobeny tím, že databáze je velmi vytížená (například vysoké zatížení procesoru). Pokud se však databáze jinak jeví jako stabilní, vysoké čekací doby naznačují potřebu zvýšit počet připojení. V konfiguraci je horní hranicí, kolik připojení může každá instance graf uzlu používat, a graf uzel nebude udržovat otevřená připojení, pokud je nepotřebuje.

Více informací o konfiguraci obchodu [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#configuring-multiple-databases).

#### Vyhrazené zpracování bloků

Pokud je nakonfigurováno více uzlů, je nutné určit jeden uzel, který je zodpovědný za přijímání nových bloků, aby všechny nakonfigurované indexové uzly neprováděly dotazování hlavy řetězce. To se provádí v rámci jmenného prostoru `chains`, kde se zadává `id_uzlu`, který se má používat pro přijímání bloků:

```toml
[chains]
ingestor = "block_ingestor_node"
```

#### Podpora více sítí

Graf protokol zvyšuje počet sítí podporovaných pro indexaci odměn a existuje mnoho podgrafů indexujících nepodporované sítě, které by indexátor rád zpracoval. Soubor `config.toml` umožňuje expresivní a flexibilní konfiguraci:

- Více sítí
- Více poskytovatelů na síť (to může umožnit rozdělení zátěže mezi poskytovatele a také konfiguraci plných uzlů i archivních uzlů, přičemž Graph Node může preferovat levnější poskytovatele, pokud to daná pracovní zátěž umožňuje).
- Další údaje o poskytovateli, jako jsou funkce, ověřování a typ poskytovatele (pro experimentální podporu Firehose)

Sekce `[chains]` řídí, ke kterým poskytovatelům ethereum se graf uzel připojuje a kde jsou uloženy bloky a další metadata pro jednotlivé řetězce. Následující příklad konfiguruje dva řetězce, mainnet a kovan, přičemž bloky pro mainnet jsou uloženy ve shard vip a bloky pro kovan jsou uloženy v primárním shard. Řetězec mainnet může používat dva různé poskytovatele, zatímco kovan má pouze jednoho poskytovatele.

```toml
[chains]
ingestor = "block_ingestor_node"
[chains.mainnet]
shard = "vip"
provider = [
  { label = "mainnet1", url = "http://..", features = [], headers = { Authorization = "Bearer foo" } },
  { label = "mainnet2", url = "http://..", features = [ "archive", "traces" ] }
]
[chains.kovan]
shard = "primary"
provider = [ { label = "kovan", url = "http://..", features = [] } ]
```

Více informací o konfiguraci poskytovatele [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#configuring-ethereum-providers).

### Proměnná prostředí

Graf Uzel podporuje řadu proměnných prostředí, které mohou povolit funkce nebo změnit chování Graf Uzel. Jsou zdokumentovány [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/environment-variables.md).

### Průběžné nasazování

Uživatelé, kteří provozují škálované nastavení indexování s pokročilou konfigurací, mohou využít správu svých graf uzlů pomocí Kubernetes.

- V úložišti indexeru je [příklad odkazu Kubernetes](https://github.com/graphprotocol/indexer/tree/main/k8s)
- [Launchpad](https://docs.graphops.xyz/launchpad/intro) je sada nástrojů pro provozování Graf Protocol Indexeru v Kubernetes, kterou spravuje společnost GraphOps. Poskytuje sadu grafů Helm a CLI pro správu nasazení uzlu Graf.

### Správa uzlu graf

Vzhledem k běžícímu uzlu Graf (nebo uzlům Graf Uzel!) je pak úkolem spravovat rozmístěné podgrafy v těchto uzlech. Graf Uzel nabízí řadu nástrojů, které pomáhají se správou podgrafů.

#### Protokolování

Protokoly Graf Uzel mohou poskytnout užitečné informace pro ladění a optimalizaci Graf Uzel a konkrétních podgrafů. Graf Uzel podporuje různé úrovně protokolů prostřednictvím proměnné prostředí `GRAPH_LOG` s následujícími úrovněmi: error, warn, info, debug nebo trace.

Kromě toho nastavení `GRAPH_LOG_QUERY_TIMING` na `gql` poskytuje více podrobností o tom, jak dotazy GraphQL probíhají (i když to bude generovat velký objem protokolů).

#### Monitorování & upozornění

Graf Uzel poskytuje metriky prostřednictvím koncového bodu Prometheus na portu 8040 ve výchozím nastavení. K vizualizaci těchto metrik lze pak použít nástroj Grafana.

Úložiště indexeru poskytuje [příklad konfigurace Grafana](https://github.com/graphprotocol/indexer/blob/main/k8s/base/grafana.yaml).

#### Graphman

`graphman` je nástroj pro údržbu Graf Uzel, který pomáhá s diagnostikou a řešením různých každodenních i výjimečných úloh.

Příkaz graphman je součástí oficiálních kontejnerů a můžete jej spustit pomocí docker exec do kontejneru graph-node. Vyžaduje soubor `config.toml`.

Úplná dokumentace příkazů `graphman` je k dispozici v úložišti Graf Uzel. Viz \[/docs/graphman.md\] (https://github.com/graphprotocol/graph-node/blob/master/docs/graphman.md) v Graf Uzel `/docs`

### Práce s podgrafy

#### Stav indexování API

API pro stav indexování, které je ve výchozím nastavení dostupné na portu 8030/graphql, nabízí řadu metod pro kontrolu stavu indexování pro různé podgrafy, kontrolu důkazů indexování, kontrolu vlastností podgrafů a další.

Úplné schéma je k dispozici [zde](https://github.com/graphprotocol/graph-node/blob/master/server/index-node/src/schema.graphql).

#### Výkonnost indexování

Proces indexování má tři samostatné části:

- Získávání zajímavých událostí od zprostředkovatele
- Zpracování událostí v pořadí pomocí příslušných obslužných (to může zahrnovat volání řetězce pro zjištění stavu a načtení dat z úložiště)
- Zápis výsledných dat do úložiště

Tyto fáze jsou spojeny do potrubí (tj. mohou být prováděny paralelně), ale jsou na sobě závislé. Pokud se podgrafy indexují pomalu, bude příčina záviset na konkrétním podgrafu.

Běžné příčiny pomalého indexování:

- Čas potřebný k nalezení relevantních událostí z řetězce (zejména obsluhy volání mohou být pomalé vzhledem k závislosti na `trace_filter`)
- Vytváření velkého počtu `eth_calls` jako součást obslužných
- Velké množství interakcí s úložištěm během provádění
- Velké množství dat k uložení do úložiště
- Velký počet událostí ke zpracování
- Pomalá doba připojení k databázi u přeplněných uzlů
- Samotný poskytovatel se dostává za hlavu řetězu
- Pomalé načítání nových účtenek od poskytovatele v hlavě řetězce

Metriky indexování podgrafů mohou pomoci diagnostikovat hlavní příčinu pomalého indexování. V některých případech spočívá problém v samotném podgrafu, ale v jiných případech mohou zlepšení síťových poskytovatelů, snížení konfliktů v databázi a další zlepšení konfigurace výrazně zlepšit výkon indexování.

#### Neúspěšné podgrafy

Během indexování mohou dílčí graf selhat, pokud narazí na neočekávaná data, pokud některá komponenta nefunguje podle očekávání nebo pokud je chyba ve zpracovatelích událostí nebo v konfiguraci. Existují dva obecné typy selhání:

- Deterministická selhání: jedná se o selhání, která nebudou vyřešena opakovanými pokusy
- Nedeterministická selhání: mohou být způsobena problémy se zprostředkovatelem nebo neočekávanou chybou grafického uzlu. Pokud dojde k nedeterministickému selhání, uzel Graf zopakuje selhání obsluhy a postupně se vrátí zpět.

V některých případech může být chyba řešitelná indexátorem (například pokud je chyba důsledkem toho, že není k dispozici správný typ zprostředkovatele, přidání požadovaného zprostředkovatele umožní pokračovat v indexování). V jiných případech je však nutná změna v kódu podgrafu.

> Deterministická selhání jsou považována za "konečná" a pro selhávající blok je vygenerován důkaz indexace, zatímco nedeterministická selhání nikoli, protože podgraf může selhat a pokračovat v indexaci. V některých případech je nedeterministické označení nesprávné a podgraf chybu nikdy nepřekoná; taková selhání by měla být hlášena jako problémy v úložišti Uzel Graf.

#### Bloková a volací mezipaměť

Uzel Graf ukládá určitá data do mezipaměti v úložišti, aby se ušetřilo opětovné načítání od zprostředkovatele. Bloky jsou ukládány do mezipaměti, stejně jako výsledky `eth_calls` (ty jsou ukládány do mezipaměti jako konkrétní blok). Toto ukládání do mezipaměti může výrazně zvýšit rychlost indexování při "resynchronizaci" mírně pozměněného podgrafu.

Pokud však uzel Etherea po určitou dobu poskytoval nesprávná data, mohou se v některých případech dostat do mezipaměti, což může vést k nesprávným datům nebo neúspěšným podgrafům. V takovém případě mohou indexery použít `graphman` k vymazání otrávené cache a následnému přetočení postižených podgrafů, které pak načtou čerstvá data od (doufejme) zdravého poskytovatele.

Pokud existuje podezření na nekonzistenci blokové mezipaměti, například chybějící událost tx receipt:

1. `graphman chain list` pro zjištění názvu řetězce.
2. `graphman chain check-blocks <CHAIN> by-number <NUMBER>` zkontroluje, zda blok uložený v mezipaměti odpovídá poskytovateli, a pokud ne, odstraní blok z mezipaměti.
   1. Pokud existuje rozdíl, může být bezpečnější zkrátit celou mezipaměť pomocí `graphman chain truncate <CHAIN>`.
   2. Pokud se blok shoduje s poskytovatelem, lze problém ladit přímo proti poskytovateli.

#### Problémy a chyby při dotazování

Jakmile je podgraf indexován, lze očekávat, že indexery budou obsluhovat dotazy prostřednictvím koncového bodu vyhrazeného pro dotazy podgrafu. Pokud indexátor doufá, že bude obsluhovat značný objem dotazů, doporučuje se použít vyhrazený uzel pro dotazy a v případě velmi vysokého objemu dotazů mohou indexátory chtít nakonfigurovat oddíly replik tak, aby dotazy neovlivňovaly proces indexování.

I s vyhrazeným dotazovacím uzlem a replikami však může provádění některých dotazů trvat dlouho a v některých případech může zvýšit využití paměti a negativně ovlivnit dobu dotazování ostatních uživatelů.

Neexistuje jedna "stříbrná kulka", ale celá řada nástrojů pro prevenci, diagnostiku a řešení pomalých dotazů.

##### Ukládání dotazů do mezipaměti

Graf Uzel ve výchozím nastavení ukládá dotazy GraphQL do mezipaměti, což může výrazně snížit zatížení databáze. To lze dále konfigurovat pomocí nastavení `GRAPH_QUERY_CACHE_BLOCKS` a `GRAPH_QUERY_CACHE_MAX_MEM` - více informací [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/environment-variables.md#graphql-caching).

##### Analýza dotazů

Problematické dotazy se nejčastěji objevují jedním ze dvou způsobů. V některých případech uživatelé sami hlásí, že daný dotaz je pomalý. V takovém případě je úkolem diagnostikovat příčinu pomalosti - zda se jedná o obecný problém, nebo o specifický problém daného podgrafu či dotazu. A pak ho samozřejmě vyřešit, pokud je to možné.

V jiných případech může být spouštěcím faktorem vysoké využití paměti v uzlu dotazu a v takovém případě je třeba nejprve identifikovat dotaz, který problém způsobuje.

Indexery mohou používat [qlog](https://github.com/graphprotocol/qlog/) ke zpracování a shrnutí protokolů dotazů uzlu Graf. Lze také povolit funkci `GRAPH_LOG_QUERY_TIMING`, která pomáhá identifikovat a ladit pomalé dotazy.

Při pomalém dotazu mají indexátory několik možností. Samozřejmě mohou změnit svůj nákladový model a výrazně zvýšit náklady na odeslání problematického dotazu. To může vést ke snížení četnosti tohoto dotazu. To však často neřeší hlavní příčinu problému.

##### Optimalizace podobná účtu

Zdá se, že databázové tabulky, které uchovávají entity, se obecně vyskytují ve dvou variantách: Jsou to tabulky "transakční", kde entity, jakmile jsou jednou vytvořeny, nejsou nikdy aktualizovány, tj. ukládají něco podobného seznamu finančních transakcí, a "účetní", kde jsou entity aktualizovány velmi často, tj. ukládají něco podobného finančním účtům, které se mění při každé zaznamenané transakci. Tabulky podobné účtům se vyznačují tím, že obsahují velké množství verzí entit, ale relativně málo odlišných entit. Často je v takových tabulkách počet odlišných entit 1 % z celkového počtu řádků (verzí entit)

Pro tabulky podobné účtům může `graph-node` generovat dotazy, které využívají podrobnosti o tom, jak Postgres ukládá data s tak vysokou rychlostí změn, totiž že všechny verze pro poslední bloky jsou v malé podsekci celkového úložiště takové tabulky.

Příkaz `graphman stats show <sgdNNNN`> zobrazí pro každý typ entity/tabulku v nasazení, kolik různých entit a kolik verzí entit každá tabulka obsahuje. Tyto údaje jsou založeny na interních odhadech společnosti Postgres, a proto jsou nutně nepřesné a mohou se lišit o řád. Hodnota `-1` ve sloupci `entity` znamená, že Postgres se domnívá, že všechny řádky obsahují odlišnou entity.

Obecně platí, že tabulky, kde počet odlišných entit tvoří méně než 1 % celkového počtu řádků/verzí entit, jsou vhodnými kandidáty pro optimalizaci podobnou účtu. Pokud výstup `graphman stats show` naznačuje, že by tabulka mohla mít z této optimalizace prospěch, spuštění `graphman stats show <sgdNNN> <table>` provede úplné spočítání tabulky - to může být pomalé, ale poskytuje přesné měření poměru odlišných entit k celkovému počtu verzí entit.

Jakmile je tabulka určena jako tabulka typu account, spuštěním příkazu `graphman stats account-like <sgdNNN>.<table>` se zapne optimalizace typu account pro dotazy proti této tabulce. Optimalizaci lze opět vypnout příkazem `graphman stats account-like --clear <sgdNNN>.<table>` Trvá až 5 minut, než si uzly dotazů všimnou, že byla optimalizace zapnuta nebo vypnuta. Po zapnutí optimalizace je nutné ověřit, zda změna skutečně nezpůsobí zpomalení dotazů pro danou tabulku. Pokud jste nakonfigurovali Grafanu pro monitorování Postgresu, pomalé dotazy by se objevily v `pg_stat_activity` ve velkých číslech a trvaly by několik sekund. V takovém případě je třeba optimalizaci opět vypnout.

U podgrafů typu Uniswap jsou tabulky `pair` a `token` hlavními kandidáty na tuto optimalizaci a mohou mít výrazný vliv na zatížení databáze.

#### Odstranění podgrafů

> Jedná se o novou funkci, která bude k dispozici v uzlu Graf 0.29.x

V určitém okamžiku může indexátor chtít daný podgraf odstranit. To lze snadno provést pomocí `graphman drop`, který odstraní nasazení a všechna jeho indexovaná data. Nasazení lze zadat buď jako název podgrafu, nebo jako hash IPFS `Qm..`, nebo jako jmenný prostor databáze `sgdNNN`. Další dokumentace je k dispozici [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/graphman.md#-drop).
