---
title: Operating Graph Node
---

Graph Node is the component which indexes subgraphs, and makes the resulting data available to query via a GraphQL API. As such it is central to the indexer stack, and correct operation of Graph Node is crucial to running a successful indexer.

这提供了Graph节点的背景概述，以及索引人可用的一些更高级的选项。可以在[Graph 节点存储库](https://github.com/graphprotocol/graph-node)中找到详细的文档和说明。

## Graph 节点

[Graph Node](https://github.com/graphprotocol/graph-node)是Graph Network上索引子图、连接到区块链客户端、索引子图并使索引数据可供查询的参考实现。

Graph Node（以及整个索引人堆栈）可以在裸机上运行，也可以在云环境中运行。中央索引组件的这种灵活性对于图形协议的有力性至关重要。类似地，Graph Node可以从[源代码构建](https://github.com/graphprotocol/graph-node)，或者索引人可以使用[提供的Docker Images](https://hub.docker.com/r/graphprotocol/graph-node)之一。

#### PostgreSQL 数据库

Graph节点的主存储区，这是存储子图数据、子图元数据以及子图不可知的网络数据（如区块缓存和eth_call缓存）的地方。

#### 网络客户端

In order to index a network, Graph Node needs access to a network client via an EVM-compatible JSON-RPC API. This RPC may connect to a single client or it could be a more complex setup that load balances across multiple.

While some subgraphs may just require a full node, some may have indexing features which require additional RPC functionality. Specifically subgraphs which make `eth_calls` as part of indexing will require an archive node which supports [EIP-1898](https://eips.ethereum.org/EIPS/eip-1898), and subgraphs with `callHandlers`, or `blockHandlers` with a `call` filter, require `trace_filter` support ([see trace module documentation here](https://openethereum.github.io/JSONRPC-trace-module)).

**Upcoming: Network Firehoses** - a Firehose is a gRPC service providing an ordered, yet fork-aware, stream of blocks, developed by The Graph's core developers to better support performant indexing at scale. This is not currently an indexer requirement, but Indexers are encouraged to familiarise themselves with the technology, ahead of full network support. Learn more about the Firehose [here](https://firehose.streamingfast.io/).

#### IPFS节点

子图部署元数据存储在IPFS网络上。Graph节点主要在子图部署期间访问IPFS节点，以获取子图清单和所有链接文件。网络索引者不需要托管自己的IPFS节点。网络的IPFS节点托管于https://ipfs.network.thegraph.com。

#### Prometheus metrics server

To enable monitoring and reporting, Graph Node can optionally log metrics to a Prometheus metrics server.

### Getting started from source

#### 安装先决条件

- **Rust**

- **PostgreSQL**

- **IPFS**

- **Additional Requirements for Ubuntu users** - To run a Graph Node on Ubuntu a few additional packages may be needed.

```sh
sudo apt-get install -y clang libpg-dev libssl-dev pkg-config
```

#### 设置

1. Start a PostgreSQL database server

```sh
initdb -D .postgres
pg_ctl -D .postgres -l logfile start
createdb graph-node
```

2. Clone [Graph Node](https://github.com/graphprotocol/graph-node) repo and build the source by running `cargo build`

3. Now that all the dependencies are setup, start the Graph Node:

```sh
cargo run -p graph-node --release -- \
  --postgres-url postgresql://[USERNAME]:[PASSWORD]@localhost:5432/graph-node \
  --ethereum-rpc [NETWORK_NAME]:[URL] \
  --ipfs https://ipfs.network.thegraph.com
```

### Getting started using Docker

#### 先决条件

- **以太坊节点**-在默认情况下，docker compose设置将使用mainnet：[http://host.docker.internal:8545](http://host.docker.internal:8545)连接到主机上的以太坊节点。您可以通过更新`docker-compose.yml`来替换此网络名称和url。

#### 设置

1. Clone Graph Node and navigate to the Docker directory:

```sh
git clone http://github.com/graphprotocol/graph-node
cd graph-node/docker
```

2. 仅适用于 linux 用户 - 在`docker-compose.yaml`中使用主机 IP 地址代替 `host.docker.internal`并使用附带的脚本。

```sh
./setup.sh
```

3. 启动一个本地 Graph 节点，它将连接到你的以太坊端点。

```sh
docker-compose up
```

### Kubernetes入门

完整的Kubernetes示例配置可以在[索引人存储库](https://github.com/graphprotocol/indexer/tree/main/k8s)中找到。

### 端口

当运行Graph Node时，会暴露以下端口：

| 端口   | 用途                                   | 路径                                                                  | CLI 参数            | Environment Variable |
| ---- | ------------------------------------ | ------------------------------------------------------------------- | ----------------- | -------------------- |
| 8000 | GraphQL HTTP 服务 <br />（用于子图查询） | /subgraphs/id/... <br /> <br /> /subgraphs/name/.../... | --http-port       | -                    |
| 8001 | GraphQL WS <br />（用于子图订阅）      | /subgraphs/id/... <br /> <br /> /subgraphs/name/.../... | --ws-port         | -                    |
| 8020 | JSON-RPC <br />（用于管理部署）        | /                                                                   | --admin-port      | -                    |
| 8030 | Subgraph indexing status API         | /graphql                                                            | --index-node-port | -                    |
| 8040 | Prometheus 指标                        | /metrics                                                            | --metrics-port    | -                    |

> **重要**: 公开暴露端口时要小心 - **管理端口** 应保持锁定。 这包括下面详述的 Graph 节点 JSON-RPC 和索引人管理端点。

# Advanced Graph Node configuration

最简单的是，Graph节点可以使用Graph节点的单个实例、单个PostgreSQL数据库、IPFS节点和要索引的子图所需的网络客户端来操作。

This setup can be scaled horizontally, by adding multiple Graph Nodes, and multiple databases to support those Graph Nodes. Advanced users may want to take advantage of some of the horizontal scaling capabilities of Graph Node, as well as some of the more advanced configuration options, via the `config.toml` file and Graph Node's environment variables.

## config.toml

[TOML](https://toml.io/en/)配置文件可用于设置比CLI中公开的配置更复杂的配置。文件的位置通过--config命令行开关传递。</p> 



> 使用配置文件时，不能使用选项--postgres-url、--postgres-secondary-hosts和--postgres-host-weights。

可以提供最小的`config.toml`文件；以下文件等效于使用--postgres-url命令行选项：



```toml
[store]
[store.primary]
connection="<.. postgres-url argument ..>"
[deployment]
[[deployment.rule]]
indexers = [ "<.. list of all indexing nodes ..>" ]
```


`config.toml`的完整文档可以在[Graph Node文档](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md)中找到。



### Multiple Graph Nodes

Graph节点索引可以水平扩展，运行Graph节点的多个实例以在不同节点之间拆分索引和查询。这只需在启动时运行配置有不同`node_id`的Graph Nodes（例如Docker Compose文件中）即可完成，然后可以在`config.toml`文件中使用该节点来[指定专用查询节点](#dedicated-query-nodes)、[区块摄取器](#dedicated-block-ingestor)，并使用[部署规则](#deployment-rules)在节点之间拆分子图。



> Note that multiple Graph Nodes can all be configured to use the same database, which itself can be horizontally scaled via sharding.



### 部署规则

Given multiple Graph Nodes, it is necessary to manage deployment of new subgraphs so that the same subgraph isn't being indexed by two different nodes, which would lead to collisions. This can be done by using deployment rules, which can also specify which `shard` a subgraph's data should be stored in, if database sharding is being used. Deployment rules can match on the subgraph name and the network that the deployment is indexing in order to make a decision.

部署规则配置示例：



```toml
[deployment]
[[deployment.rule]]
match = { name = "(vip|important)/.*" }
shard = "vip"
indexers = [ "index_node_vip_0", "index_node_vip_1" ]
[[deployment.rule]]
match = { network = "kovan" }
# No shard, so we use the default shard called 'primary'
indexers = [ "index_node_kovan_0" ]
[[deployment.rule]]
match = { network = [ "xdai", "poa-core" ] }
indexers = [ "index_node_other_0" ]
[[deployment.rule]]
# There's no 'match', so any subgraph matches
shards = [ "sharda", "shardb" ]
indexers = [
    "index_node_community_0",
    "index_node_community_1",
    "index_node_community_2",
    "index_node_community_3",
    "index_node_community_4",
    "index_node_community_5"
  ]
```


在[此处](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#controlling-deployment)阅读有关部署规则的更多信息。



### 专用查询节点

通过在配置文件中包含以下内容，可以将节点配置为显式查询节点：



```toml
[general]
query = "<regular expression>"
```


任何--node-id与正则表达式匹配的节点都将被设置为只响应查询。



### 通过分片扩展数据库

For most use cases, a single Postgres database is sufficient to support a graph-node instance. When a graph-node instance outgrows a single Postgres database, it is possible to split the storage of graph-node's data across multiple Postgres databases. All databases together form the store of the graph-node instance. Each individual database is called a shard.

Shards can be used to split subgraph deployments across multiple databases, and can also be used to use replicas to spread query load across databases. This includes configuring the number of available database connections each `graph-node` should keep in its connection pool for each database, which becomes increasingly important as more subgraphs are being indexed.

Sharding becomes useful when your existing database can't keep up with the load that Graph Node puts on it, and when it's not possible to increase the database size anymore.



> It is generally better make a single database as big as possible, before starting with shards. One exception is where query traffic is split very unevenly between subgraphs; in those situations it can help dramatically if the high-volume subgraphs are kept in one shard and everything else in another because that setup makes it more likely that the data for the high-volume subgraphs stays in the db-internal cache and doesn't get replaced by data that's not needed as much from low-volume subgraphs.

In terms of configuring connections, start with max_connections in postgresql.conf set to 400 (or maybe even 200) and look at the store_connection_wait_time_ms and store_connection_checkout_count Prometheus metrics. Noticeable wait times (anything above 5ms) is an indication that there are too few connections available; high wait times there will also be caused by the database being very busy (like high CPU load). However if the database seems otherwise stable, high wait times indicate a need to increase the number of connections. In the configuration, how many connections each graph-node instance can use is an upper limit, and Graph Node will not keep connections open if it doesn't need them.

在[此处](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#configuring-multiple-databases)阅读有关存储配置的更多信息。



### 专用区块摄取

如果配置了多个节点，则需要指定一个负责接收新区块的节点，这样所有配置的索引节点都不会轮询链头。这是作为`chains`命名空间的一部分完成的，指定用于区块摄取的`node_id`：



```toml
[chains]
ingestor = "block_ingestor_node"
```




### 支持多个网络

Graph协议正在增加支持索引奖励的网络数量，并且存在许多索引不支持的网络的子图，索引人希望处理这些子图。`config.toml`文件允许表达和灵活配置：

- 多个网络。
- Multiple providers per network (this can allow splitting of load across providers, and can also allow for configuration of full nodes as well as archive nodes, with Graph Node preferring cheaper providers if a given workload allows).
- 其他提供商详细信息，如特征、身份验证和提供程序类型（用于实验性Firehose支持）。

The `[chains]` section controls the ethereum providers that graph-node connects to, and where blocks and other metadata for each chain are stored. The following example configures two chains, mainnet and kovan, where blocks for mainnet are stored in the vip shard and blocks for kovan are stored in the primary shard. The mainnet chain can use two different providers, whereas kovan only has one provider.



```toml
[chains]
ingestor = "block_ingestor_node"
[chains.mainnet]
shard = "vip"
provider = [
  { label = "mainnet1", url = "http://..", features = [], headers = { Authorization = "Bearer foo" } },
  { label = "mainnet2", url = "http://..", features = [ "archive", "traces" ] }
]
[chains.kovan]
shard = "primary"
provider = [ { label = "kovan", url = "http://..", features = [] } ]
```


在[此处](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#configuring-ethereum-providers)阅读有关存储配置的更多信息。



## Environment variables

Graph节点支持一系列环境变量，这些变量可以启用特征或更改Graph节点的行为。这些都记录在[这里](https://github.com/graphprotocol/graph-node/blob/master/docs/environment-variables.md)。



## 持续部署

使用高级配置操作缩放索引设置的用户可以从使用Kubernetes管理Graph节点中受益。

- 索引者存储库有一个示例[Kubernetes引用](https://github.com/graphprotocol/indexer/tree/main/k8s)
- [Launchpad](https://docs.graphops.xyz/launchpad/intro)是一个工具包，用于在GraphOps维护的Kubernetes上运行Graph Protocol Indexer。它提供了一组Helm图表和一个CLI来管理Graph Node部署。



## 管理Graph节点

Given a running Graph Node (or Graph Nodes!), the challenge is then to manage deployed subgraphs across those nodes. Graph Node surfaces a range of tools to help with managing subgraphs.



### 日志

Graph节点的日志可以为Graph节点和特定子图的调试和优化提供有用的信息。Graph节点通过`GRAPH_LOG`环境变量支持不同的日志级别，具有以下级别：错误、警告、信息、调试或跟踪。

此外，将`GRAPH_LOG_QUERY_TIMING`设置为`gql`提供了有关GraphQL查询如何运行的更多详细信息（尽管这将生成大量日志）。



### 监控& 警报

Graph Node provides the metrics via Prometheus endpoint on 8040 port by default. Grafana can then be used to visualise these metrics.

索引人存储库提供了[Grafana配置示例](https://github.com/graphprotocol/indexer/blob/main/k8s/base/grafana.yaml)。



### Graphman

`graphman`是Graph节点的维护工具，帮助诊断和解决不同的日常和异常任务。

The graphman command is included in the official containers, and you can docker exec into your graph-node container to run it. It requires a `config.toml` file.

Graph节点存储库中提供了`graphman`命令的完整文档。参见\[/docs/graphman.md\] (https://github.com/graphprotocol/graph-node/blob/master/docs/graphman.md)Graph节点`/docs`



## 使用子图



### 索引状态API

默认情况下，在端口8030/graphql上可用，索引状态API公开了一系列方法，用于检查不同子图的索引状态、检查索引证明、检查子图特征等。

[此处](https://github.com/graphprotocol/graph-node/blob/master/server/index-node/src/schema.graphql)提供完整的模式。



### 索引性能

索引过程有三个独立的部分：

- 正在从提供程序获取感兴趣的事件
- 使用适当的处理程序按顺序处理事件（这可能涉及调用状态链，并从存储中获取数据）
- 将生成的数据写入存储

这些阶段是流水线的（即可以并行执行），但它们彼此依赖。如果子图索引速度慢，则根本原因将取决于特定的子图。

索引速度慢的常见原因：

- 从链中查找相关事件所需的时间（特别是调用处理程序可能慢，因为依赖`trace_filter`）
- 将大量`eth_calls`作为处理程序的一部分
- 执行期间大量的存储交互
- 要保存到存储的大量数据
- 要处理的大量事件
- 拥挤节点的数据库连接时间慢
- 提供商本身落后于链头
- 在链头从提供商获取新收据的速度较慢

Subgraph indexing metrics can help diagnose the root cause of indexing slowness. In some cases, the problem lies with the subgraph itself, but in others, improved network providers, reduced database contention and other configuration improvements can markedly improve indexing performance.



### 失败的子图

在为子图编制索引期间，如果遇到意外数据、某些组件未按预期工作，或者事件处理程序或配置中存在错误，则子图可能会失败。有两种常见的故障类型：

- 确定性故障：这些故障不会通过重试解决
- 非确定性故障：这些故障可能是由于提供程序的问题，或者是一些意外的Graph节点错误。当发生非确定性故障时，Graph节点将重试失败的处理程序，并随着时间的推移而后退。

在某些情况下，索引人可能会解决故障（例如，如果错误是由于没有正确类型的提供程序导致的，则添加所需的提供程序将允许继续索引）。然而，在其他情况下，需要更改子图代码。



> 确定性故障被视为“最终”，并为故障区块生成索引证明，而非确定性故障则不是，因为子图可能会“可靠”并继续索引。在某些情况下，非确定性标签是不正确的，子图永远无法克服错误；此类故障应报告为Graph节点存储库中的问题。



### Block and call cache

Graph节点在存储中缓存某些数据，以保存来自提供程序的重新绘制。区块被缓存，`eth_calls`的结果也被缓存（后者作为特定区块被缓存）。这种缓存可以在稍微改变的子图的“重新同步”期间显著提高索引速度。

However in some instances, if an Ethereum node has provided incorrect data for some period, that can make its way into the cache, leading to incorrect data or failed subgraphs. In this case indexers can use `graphman` to clear the poisoned cache, and then rewind the affected subgraphs, which will then fetch fresh data from the (hopefully) healthy provider.

如果怀疑区块缓存不一致，例如tx收据丢失事件：

1. `graphman链列表`以查找链名称。
2. `graphman chain check-blocks <CHAIN> by-number <NUMBER>` 将检查缓存的块是否与提供程序匹配，如果不匹配，则从缓存中删除该块。 
      1. 如果存在差异，则使用`graphman chain truncate <CHAIN>`截断整个缓存可能更安全。
   2. 如果区块与提供程序匹配，则可以直接针对提供程序调试问题。



### 查询问题和错误

Once a subgraph has been indexed, indexers can expect to serve queries via the subgraph's dedicated query endpoint. If the indexer is hoping to serve significant query volume, a dedicated query node is recommended, and in case of very high query volumes, indexers may want to configure replica shards so that queries don't impact the indexing process.

然而，即使使用专用的查询节点和副本，某些查询也可能需要很长时间才能执行，在某些情况下还会增加内存使用量，并对其他用户的查询时间产生负面影响。

There is not one "silver bullet", but a range of tools for preventing, diagnosing and dealing with slow queries.



#### 查询缓存

默认情况下，Graph 节点缓存GraphQL查询，这可以显著减少数据库负载。这可以进一步使用`GRAPH_QUERY_CACHE_BLOCKS`和`GRAPH_QUERY_CACHE-MAX_MEM`设置进行配置-请在[此处](https://github.com/graphprotocol/graph-node/blob/master/docs/environment-variables.md#graphql-caching)阅读更多信息。



#### 分析查询

有问题的查询通常以两种方式之一出现。在某些情况下，用户自己报告给定的查询很慢。在这种情况下，挑战是诊断缓慢的原因——无论是一般问题，还是特定于子图或查询。如果可能的话，当然要解决它。

在其他情况下，触发因素可能是查询节点上的高内存使用率，在这种情况下，首要挑战是要确定导致问题的查询。

Indexers can use [qlog](https://github.com/graphprotocol/qlog/) to process and summarize Graph Node's query logs. `GRAPH_LOG_QUERY_TIMING` can also be enabled to help identify and debug slow queries.

Given a slow query, indexers have a few options. Of course they can alter their cost model, to significantly increase the cost of sending the problematic query. This may result in a reduction in the frequency of that query. However this often doesn't resolve the root cause of the issue.



#### 账户式优化

存储实体的数据库表通常有两种类型：“类交易”，即实体一旦创建，就永远不会更新，即存储类似于金融交易列表的内容；“类账户”，即经常更新实体，即存储每次记录交易时都会修改的类似金融账户的内容。类账户表的特点是，它们包含大量实体版本，但不同的实体相对较少。通常，在这种表中，不同实体的数量是行总数的1%（实体版本）。

对于类似账户的表，`graph-node` 可以生成查询，利用Postgres如何以如此高的变化率存储数据的细节，即最近区块的所有版本都位于此类表的总存储的一小部分中。

The command `graphman stats show <sgdNNNN`> shows, for each entity type/table in a deployment, how many distinct entities, and how many entity versions each table contains. That data is based on Postgres-internal estimates, and is therefore necessarily imprecise, and can be off by an order of magnitude. A `-1` in the `entities` column means that Postgres believes that all rows contain a distinct entity.

通常，不同实体的数量小于行/实体版本总数的1%的表是类似账户的优化的好候选表。当`graphman stats show`的输出表明表可能从该优化中受益时，运行`graphman stats show <sgdNNN> <table>`将执行表的完整计数-这可能会很慢，但可以精确测量不同实体与整体实体版本的比率。

Once a table has been determined to be account-like, running `graphman stats account-like <sgdNNN>.<table>` will turn on the account-like optimization for queries against that table. The optimization can be turned off again with `graphman stats account-like --clear <sgdNNN>.<table>` It takes up to 5 minutes for query nodes to notice that the optimization has been turned on or off. After turning the optimization on, it is necessary to verify that the change does not in fact make queries slower for that table. If you have configured Grafana to monitor Postgres, slow queries would show up in `pg_stat_activity`in large numbers, taking several seconds. In that case, the optimization needs to be turned off again.

对于类似Uniswap的子图，`pair` 和 `token`表是这种优化的主要候选项，并且可以对数据库负载产生显著影响。



### 删除子图



> 这是一项新功能，将在Graph Node 0.29.x中提供。

在某个时刻，索引人可能想要删除给定的子图。这可以通过删除部署及其所有索引数据的`graphman drop`, 轻松完成。部署可以被指定为子图名称、IPFS has、`Qm..`，或数据库命名空间`sgdNNN`。[此处](https://github.com/graphprotocol/graph-node/blob/master/docs/graphman.md#-drop)提供了更多文档。
