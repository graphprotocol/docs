---
title: 使用分叉快速轻松地调试子图
---

与许多处理大量数据的系统一样，Graph 的索引人(Graph 节点)可能需要相当长的时间才能实现子图从目标区块链的数据同步。 我们很清楚以调试为目的的快速更改与索引所需的长时间等待存在矛盾。 因此我们在本文中介绍由 [LimeChain](https://limechain.tech/) 开发的 **subgraph 分叉** ，并且展示如何使用此功能来加快子图调试！

## 首先，让我们来看什么是子图分叉

**Subgraph forking** is the process of lazily fetching entities from _another_ subgraph's store (usually a remote one).

在调试时，**subgraph forking** 允许您在固定的区块 _X_ 中调试失败的子图，而无需等待区块同步 _X_。

## 让我们看看这是如何做到的？

当您将子图部署到远程 Graph 节点进行索引而它在区块 _X_上失败时，好消息是Graph 节点仍能以同步至区块 _X_的数据响应GraphQL 查询。这意味着我们可以基于区块 _X_ 最新的数据来修复索引时出现的错误。

简而言之，我们将从远程 Graph 节点 _分叉失败的子图_，保证子图索引更新至区块 *X*的数据， 以便基于更新至区块 _X_ 的数据在本地部署的子图进行调试，以反映索引数据的最新状态。

## 下面让我们看一下代码示例

为了简单，让我们聚焦子图调试的场景，以索引 Ethereum Gravity 智能合约的 [示例子图](https://github.com/graphprotocol/example-subgraphs/tree/main/ethereum/gravatar) 来看代码。

Here are the handlers defined for indexing `Gravatar`s, with no bugs whatsoever:

```tsx
export function handleNewGravatar(event: NewGravatar): void {
  let gravatar = new Gravatar(event.params.id.toHex().toString())
  gravatar.owner = event.params.owner
  gravatar.displayName = event.params.displayName
  gravatar.imageUrl = event.params.imageUrl
  gravatar.save()
}

export function handleUpdatedGravatar(event: UpdatedGravatar): void {
  let gravatar = Gravatar.load(event.params.id.toI32().toString())
  if (gravatar == null) {
    log.critical('Gravatar not found!', [])
    return
  }
  gravatar.owner = event.params.owner
  gravatar.displayName = event.params.displayName
  gravatar.imageUrl = event.params.imageUrl
  gravatar.save()
}
```

Oops, how unfortunate, when I deploy my perfect looking subgraph to the [Hosted Service](https://thegraph.com/hosted-service/) it fails with the _"Gravatar not found!"_ error.

尝试修复的常用方法是：

1. Make a change in the mappings source, which you believe will solve the issue (while I know it won't).
2. Re-deploy the subgraph to the [Hosted Service](https://thegraph.com/hosted-service/) (or another remote Graph node).
3. Wait for it to sync-up.
4. If it breaks again go back to 1, otherwise: Hooray!

It is indeed pretty familiar to an ordinary debug process, but there is one step that horribly slows down the process: _3. Wait for it to sync-up._

Using **subgraph forking** we can essentially eliminate this step. Here is how it looks:

0. Spin-up a local Graph node with the **_appropriate fork-base_** set.
1. 按照你认为可以解决问题的方法，在映射源中进行更改。
2. Deploy to the local Graph node, **_forking the failing subgraph_** and **_starting from the problematic block_**.
3. If it breaks again, go back to 1, otherwise: Hooray!

Now, you may have 2 questions:

1. fork-base what???
2. Forking who?!

And I answer:

1. `子分叉集` 是“基础”URL，例如将 _子图 id_ 添加到结果 URL (`<子分叉集>/<subgraph-id>`) ，就是一个合法的子图GraphQL访问端口。
2. 分叉容易，不要紧张：

```bash
$ graph deploy <subgraph-name> --debug-fork <subgraph-id> --ipfs http://localhost:5001 --node http://localhost:8020
```

Also, don't forget to set the `dataSources.source.startBlock` field in the subgraph manifest to the number of the problematic block, so you can skip indexing unnecessary blocks and take advantage of the fork!

所以，我是这么做的：

0. 我启动一个本地Graph节点（[这里是如何做到的](https://github.com/graphprotocol/graph-node#running-a-local-graph-node)） 将 `子分叉集` 选项设为：`https://api.thegraph.com/subgraphs/id/`，因为我将从 [托管服务](https://thegraph.com/hosted-service/)分叉子图，即之前部署有问题的子图。

```
$ cargo run -p graph-node --release -- \
    --postgres-url postgresql://USERNAME[:PASSWORD]@localhost:5432/graph-node \
    --ethereum-rpc NETWORK_NAME:[CAPABILITIES]:URL \
    --ipfs 127.0.0.1:5001
    --fork-base https://api.thegraph.com/subgraphs/id/
```

1. After careful inspection I notice that there is a mismatch in the `id` representations used when indexing `Gravatar`s in my two handlers. While `handleNewGravatar` converts it to a hex (`event.params.id.toHex()`), `handleUpdatedGravatar` uses an int32 (`event.params.id.toI32()`) which causes the `handleUpdatedGravatar` to panic with "Gravatar not found!". I make them both convert the `id` to a hex.
2. After I made the changes I deploy my subgraph to the local Graph node, **_forking the failing subgraph_** and setting `dataSources.source.startBlock` to `6190343` in `subgraph.yaml`:

```bash
$ graph deploy gravity --debug-fork QmNp169tKvomnH3cPXTfGg4ZEhAHA6kEq5oy1XDqAxqHmW --ipfs http://localhost:5001 --node http://localhost:8020
```

3. I inspect the logs produced by the local Graph node and, Hooray!, everything seems to be working.
4. I deploy my now bug-free subgraph to a remote Graph node and live happily ever after! (no potatoes tho)
5. 结束
