---
title: Erweiterte Subgraph-Funktionen
---

## Überblick

Fügen Sie fortgeschrittene Subgraph-Funktionen hinzu und implementieren Sie sie, um Ihre Subgraphen zu verbessern.

Ab `specVersion` `0.0.4` müssen Subgraph-Funktionen explizit im Abschnitt `features` auf der obersten Ebene der Manifestdatei unter Verwendung ihres `camelCase`-Namens deklariert werden, wie in der folgenden Tabelle aufgeführt:

| Funktion                                          | Name             |
| ------------------------------------------------- | ---------------- |
| [Non-fatal errors](#non-fatal-errors)             | `nonFatalErrors` |
| [Volltextsuche](#defining-fulltext-search-fields) | "Volltextsuche"  |
| [Grafting](#grafting-onto-existing-subgraphs)     | `grafting`       |

Wenn ein Subgraph beispielsweise die Funktionen **Volltextsuche** und **Nicht fatale Fehler** verwendet, sollte das Feld „Features“ im Manifest lauten:

```yaml
specVersion: 1.3.0
Beschreibung: Gravatar für Ethereum
Funktionen:
  - fullTextSearch
  - nonFatalErrors
Datenquellen: ...
```

> Beachten Sie, dass die Verwendung einer Funktion ohne deren Deklaration zu einem **Validierungsfehler** beim Einsatz von Subgraphen führt, aber keine Fehler auftreten, wenn eine Funktion deklariert, aber nicht verwendet wird.

## Subgraph Best Practice 5: Timeseries and Aggregations

Voraussetzungen:

- Subgraph specVersion muss ≥1.1.0 sein.

Zeitreihen und Aggregationen ermöglichen es Ihrem Subgraph, Statistiken wie den täglichen Durchschnittspreis, stündliche Gesamttransfers und mehr zu verfolgen.

Mit dieser Funktion werden zwei neue Typen von Subgraph-Entitäten eingeführt. Zeitreihen-Entitäten zeichnen Datenpunkte mit Zeitstempeln auf. Aggregations-Entitäten führen vordeklarierte Berechnungen an den Zeitreihen-Datenpunkten auf stündlicher oder täglicher Basis durch und speichern dann die Ergebnisse für den einfachen Zugriff über GraphQL.

### Beispiel-Schema

```graphql
type Data @entity(timeseries: true) {
  id: Int8!
  timestamp: Timestamp!
  price: BigDecimal!
}

type Stats @aggregation(intervals: [„hour“, „day“], source: „Data“) {
  id: Int8!
  timestamp: Timestamp!
  sum: BigDecimal! @aggregate(fn: „sum“, arg: „price“)
}
```

### Definition von Zeitreihen und Aggregationen

Zeitreihenentitäten werden mit `@entity(timeseries: true)` im GraphQL-Schema definiert. Jede Zeitreihen-Entität muss:

- haben eine eindeutige ID vom Typ int8
- einen Zeitstempel vom Typ Zeitstempel haben
- Daten enthalten, die von den Aggregationseinheiten für die Berechnung verwendet werden.

Diese Timeseries-Entitäten können in regulären Trigger-Handlern gespeichert werden und dienen als „Rohdaten“ für die Aggregationsentitäten.

Aggregation entities are defined with `@aggregation` in the GraphQL schema. Every aggregation entity defines the source from which it will gather data (which must be a timeseries entity), sets the intervals (e.g., hour, day), and specifies the aggregation function it will use (e.g., sum, count, min, max, first, last).

Die Aggregationseinheiten werden automatisch auf der Grundlage der angegebenen Quelle am Ende des gewünschten Intervalls berechnet.

#### Verfügbare Aggregationsintervalle

- Stunde": setzt den Zeitraum der Zeitreihe stündlich, zur vollen Stunde.
- Tag": legt den Zeitraum der Zeitreihe für jeden Tag fest, beginnend und endend um 00:00 Uhr.

#### Verfügbare Aggregationsfunktionen

- Summe": Summe aller Werte.
- Anzahl": Anzahl der Werte.
- Min": Minimaler Wert.
- `max`: Maximaler Wert.
- "erster": Erster Wert in der Periode.
- Letzter Wert: Letzter Wert in der Periode.

#### Beispiel-Aggregationsabfrage

```graphql
{
  stats(interval: „hour“, where: { timestamp_gt: 1704085200 }) {
    id
    Zeitstempel
    Summe
  }
}
```

[Lesen Sie mehr](https://github.com/graphprotocol/graph-node/blob/master/docs/aggregations.md) über Zeitreihen und Aggregationen.

## Non-fatal errors

Indexierungsfehler bei bereits synchronisierten Subgraphen führen standardmäßig dazu, dass der Subgraph fehlschlägt und die Synchronisierung beendet wird. Subgraphen können alternativ so konfiguriert werden, dass die Synchronisierung bei Fehlern fortgesetzt wird, indem die vom Handler, der den Fehler verursacht hat, vorgenommenen Änderungen ignoriert werden. Dies gibt den Autoren von Untergraphen Zeit, ihre Subgraphen zu korrigieren, während die Abfragen weiterhin gegen den letzten Block ausgeführt werden, obwohl die Ergebnisse aufgrund des Fehlers, der den Fehler verursacht hat, inkonsistent sein könnten. Beachten Sie, dass einige Fehler immer noch fatal sind. Um nicht fatal zu sein, muss der Fehler als deterministisch bekannt sein.

> **Hinweis:** Das The Graph Netzwerk unterstützt noch keine nicht-fatalen Fehler, und Entwickler sollten keine Subgraphen, die diese Funktionalität nutzen, über das Studio im Netzwerk bereitstellen.

Zur Aktivierung von nicht schwerwiegenden Fehlern muss das folgende Funktionskennzeichen im Manifest des Subgraphen gesetzt werden:

```yaml
specVersion: 1.3.0
Beschreibung: Gravatar für Ethereum
Merkmale:
    - nonFatalErrors
    ...
```

Die Abfrage muss sich auch für die Abfrage von Daten mit potenziellen Inkonsistenzen durch das Argument `subgraphError` entscheiden. Es wird auch empfohlen, `_meta` abzufragen, um zu prüfen, ob der Subgraph Fehler übersprungen hat, wie in diesem Beispiel:

```graphql
foos(first: 100, subgraphError: allow) {
 id
}

_meta {
 hasIndexingErrors
}
```

Wenn der Subgraph auf einen Fehler stößt, gibt diese Abfrage sowohl die Daten als auch einen Graphql-Fehler mit der Meldung `„indexing_error“` zurück, wie in dieser Beispielantwort:

```graphql
"Daten": {
 "foos": [
 {
 "id": "0xdead"
 }
 ],
 "_meta": {
 "hasIndexingErrors": true
 }
},
"errors": [
 {
 "message": "indexing_error"
 }
]
```

## IPFS/Arweave File Datenquellen

Dateidatenquellen sind eine neue Subgraph-Funktionalität für den Zugriff auf Off-Chain-Daten während der Indizierung in einer robusten, erweiterbaren Weise. Dateidatenquellen unterstützen das Abrufen von Dateien aus dem IPFS und aus Arweave.

> Damit wird auch die Grundlage für die deterministische Indizierung von Off-Chain-Daten sowie für die potenzielle Einführung beliebiger HTTP-Daten geschaffen.

### Überblick

Anstatt die Dateien während der Ausführung des Handlers „in line“ zu holen, werden Vorlagen eingeführt, die als neue Datenquellen für eine bestimmte Dateikennung erzeugt werden können. Diese neuen Datenquellen rufen die Dateien ab, versuchen es erneut, wenn sie nicht erfolgreich sind, und führen einen speziellen Handler aus, wenn die Datei gefunden wird.

Dies ist vergleichbar mit den [existing data source templates](/developing/creating-a-subgraph/#data-source-templates), die zur dynamischen Erstellung neuer kettenbasierter Datenquellen verwendet werden.

> Dies ersetzt die bestehende API „ipfs.cat“.

### Upgrade-Leitfaden

#### Aktualisierung von `graph-ts` und `graph-cli`

Dateidatenquellen erfordern graph-ts &gt;=0.29.0 und graph-cli &gt;=0.33.1

#### Hinzufügen eines neuen Entitätstyps, der aktualisiert wird, wenn Dateien gefunden werden

Dateidatenquellen können nicht auf kettenbasierte Entitäten zugreifen oder diese aktualisieren, sondern müssen dateispezifische Entitäten aktualisieren.

Dies kann bedeuten, dass Felder aus bestehenden Entitäten in separate, miteinander verbundene Entitäten aufgeteilt werden.

Ursprüngliche kombinierte Einheit:

```graphql
type Token @entity {
  id: ID!
  tokenID: BigInt!
  tokenURI: String!
  externalURL: String!
  ipfsURI: String!
  image: String!
  name: String!
  description: String!
  type: String!
  updatedAtTimestamp: BigInt
  owner: User!
}
```

Neu, geteilte Einheit:

```graphql
type Token @entity {
  id: ID!
  tokenID: BigInt!
  tokenURI: String!
  ipfsURI: TokenMetadata
  updatedAtTimestamp: BigInt
  owner: String!
}

type TokenMetadata @entity {
  id: ID!
  image: String!
  externalURL: String!
  name: String!
  description: String!
}
```

Wenn die Beziehung zwischen der übergeordneten Entität und der resultierenden Dateidatenquellen-Entität 1:1 ist, besteht das einfachste Muster darin, die übergeordnete Entität mit einer resultierenden Datei-Entität zu verknüpfen, indem die IPFS CID als Lookup verwendet wird. Kontaktieren Sie uns auf Discord, wenn Sie Schwierigkeiten bei der Modellierung Ihrer neuen dateibasierten Entitäten haben!

> Sie können [nested filters](/subgraphs/querying/graphql-api/#example-for-nested-entity-filtering) verwenden, um übergeordnete Entitäten auf der Grundlage dieser verschachtelten Entitäten zu filtern.

#### Hinzufügen einer neuen Schablonen-Datenquelle mit „Art: file/ipfs“ oder „Art: file/arweave“.

Dies ist die Datenquelle, die erzeugt wird, wenn eine Datei von Interesse identifiziert wird.

```yaml
Vorlagen:
  - name: TokenMetadaten
    Art: Datei/ipfs
    mapping:
      apiVersion: 0.0.9
      Sprache: wasm/assemblyscript
      Datei: ./src/mapping.ts
      handler: handleMetadata
      Entitäten:
        - TokenMetadaten
      abis:
        - name: Token
          Datei: ./abis/Token.json
```

> Derzeit sind „abis“ erforderlich, obwohl es nicht möglich ist, Verträge aus Dateidatenquellen aufzurufen.

Die Dateidatenquelle muss alle Entitätstypen, mit denen sie interagieren wird, unter „Entitäten“ ausdrücklich erwähnen. Siehe [limitations](#limitations) für weitere Details.

#### Erstellen Sie einen neuen Handler zur Verarbeitung von Dateien

Dieser Handler sollte einen `Bytes`-Parameter akzeptieren, der den Inhalt der Datei darstellt, wenn diese gefunden wird und dann verarbeitet werden kann. Oft handelt es sich dabei um eine JSON-Datei, die mit `graph-ts`-Helfern verarbeitet werden kann ([documentation](/subgraphs/developing/creating/graph-ts/api/#json-api)).

Auf das CID der Datei als lesbare Zeichenkette kann über die `dataSource` wie folgt zugegriffen werden:

```typescript
const cid = dataSource.stringParam()
```

Beispiel-Handler:

```typescript
Importiere { json, Bytes, dataSource } von '@graphprotocol/graph-ts'
importiere { TokenMetadata } von '.. generated/schema'

export function handleMetadata(content: Bytes): void {
  let tokenMetadata = new TokenMetadata(dataSource. tringParam())
  const value = json.fromBytes(content).toObject()
  if (value) {
    const image = value. et('image')
    const name = value.get('name')
    const description = value. et('description')
    const externalURL = Wert. et('external_url')

    if (name && image && description && externalURL) {
      tokenMetadata. ame = name.toString()
      tokenMetadata. mage = image.toString()
      tokenMetadata.externalURL = externalURL.toString()
      tokenMetadata. escription = description.toString()
    }

    tokenMetadata.save()
  }
}
```

#### Dateidatenquellen bei Bedarf abrufen

Sie können jetzt Dateidatenquellen während der Ausführung von kettenbasierten Handlern erstellen:

- Importieren Sie die Vorlage aus den automatisch erzeugten „Templates“.
- Aufruf von `TemplateName.create(cid: string)` innerhalb einer Zuordnung, wobei cid ein gültiger Inhaltsbezeichner für IPFS oder Arweave ist

Für IPFS unterstützt Graph Node [v0- und v1-Inhaltsbezeichner] (https://docs.ipfs.tech/concepts/content-addressing/) und Inhaltsbezeichner mit Verzeichnissen (z. B. `bafyreighykzv2we26wfrbzkcdw37sbrby4upq7ae3aqobbq7i4er3tnxci/metadata.json`).

Für Arweave kann Graph Node ab Version 0.33.0 Dateien, die auf Arweave gespeichert sind, basierend auf ihrer [Transaktions-ID](https://docs.arweave.org/developers/arweave-node-server/http-api#transactions) von einem Arweave-Gateway ([Beispieldatei](https://bdxujjl5ev5eerd5ouhhs6o4kjrs4g6hqstzlci5pf6vhxezkgaa.arweave.net/CO9EpX0lekJEfXUOeXncUmMuG8eEp5WJHXl9U9yZUYA)) abrufen. Arweave unterstützt Transaktionen, die über Irys (früher Bundlr) hochgeladen werden, und Graph Node kann auch Dateien auf der Grundlage von [Irys-Manifesten](https://docs.irys.xyz/overview/gateways#indexing) abrufen.

Beispiel:

```typescript
import { TokenMetadata as TokenMetadataTemplate } from '../generated/templates'

const ipfshash = 'QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm'
//Dieser Beispielcode ist für einen Crypto Coven Subgraph. Der obige ipfs-Hash ist ein Verzeichnis mit Token-Metadaten für alle Crypto Coven NFTs.

export function handleTransfer(event: TransferEvent): void {
  let token = Token.load(event.params.tokenId.toString())
  if (!token) {
    token = new Token(event.params.tokenId.toString())
    token.tokenID = event.params.tokenId

    token.tokenURI = '/' + event.params.tokenId.toString() + '.json'
    const tokenIpfsHash = ipfshash + token.tokenURI
    //This erstellt einen Pfad zu den Metadaten für eine einzelne Crypto Coven NFT. Das Verzeichnis wird mit „/“ + Dateiname + „.json“ verknüpft.

    token.ipfsURI = tokenIpfsHash

    TokenMetadataTemplate.create(tokenIpfsHash)
  }

  token.updatedAtTimestamp = event.block.timestamp
  token.owner = event.params.to.toHexString()
  token.save()
}
```

Dadurch wird eine neue Dateidatenquelle erstellt, die den konfigurierten IPFS- oder Arweave-Endpunkt des Graph Node abfragt und es erneut versucht, wenn sie nicht gefunden wird. Wenn die Datei gefunden wird, wird der Dateidatenquellen-Handler ausgeführt.

In diesem Beispiel wird die CID als Lookup zwischen der übergeordneten Entität „Token“ und der daraus resultierenden Entität „TokenMetadata“ verwendet.

> Früher hätte ein Subgraph-Entwickler an dieser Stelle `ipfs.cat(CID)` aufgerufen, um die Datei zu holen

Herzlichen Glückwunsch, Sie verwenden Dateidatenquellen!

#### Einsatz von Subgraphen

Sie können jetzt Ihren Subgraphen auf jedem Graph Node >=v0.30.0-rc.0 „bauen“ und „bereitstellen“.

#### Beschränkungen

Dateidatenquellen-Handler und -Entitäten sind von anderen Subgraph-Entitäten isoliert, wodurch sichergestellt wird, dass sie bei ihrer Ausführung deterministisch sind und keine Kontamination von kettenbasierten Datenquellen erfolgt. Um genau zu sein:

- Von Dateidatenquellen erstellte Entitäten sind unveränderlich und können nicht aktualisiert werden.
- Dateidatenquellen-Handler können nicht auf Entitäten aus anderen Dateidatenquellen zugreifen
- Auf Entitäten, die mit Dateidatenquellen verknüpft sind, kann von kettenbasierten Handlern nicht zugegriffen werden

> Während diese Einschränkung für die meisten Anwendungsfälle nicht problematisch sein sollte, kann sie für einige Fälle zu mehr Komplexität führen. Bitte kontaktieren Sie uns über Discord, wenn Sie Probleme bei der Modellierung Ihrer dateibasierten Daten in einem Subgraphen haben!

Außerdem ist es nicht möglich, Datenquellen aus einer Dateidatenquelle zu erstellen, sei es eine Onchain-Datenquelle oder eine andere Dateidatenquelle. Diese Einschränkung kann in Zukunft aufgehoben werden.

#### Bewährte Praktiken

Wenn Sie NFT-Metadaten mit entsprechenden Token verknüpfen, verwenden Sie den IPFS-Hash der Metadaten, um eine Metadaten-Entität von der Token-Entität zu referenzieren. Speichern Sie die Metadaten-Entität unter Verwendung des IPFS-Hashs als ID.

Sie können [DataSource context](/subgraphs/developing/creating/graph-ts/api/#entity-and-datasourcecontext) beim Erstellen von Dateidatenquellen verwenden, um zusätzliche Informationen zu übergeben, die dem Dateidatenquellen-Handler zur Verfügung stehen.

Wenn Sie Entitäten haben, die mehrfach aktualisiert werden, erstellen Sie eindeutige dateibasierte Entitäten unter Verwendung des IPFS-Hash &amp; der Entitäts-ID, und verweisen Sie auf sie mit einem abgeleiteten Feld in der kettenbasierten Entität.

> Wir arbeiten daran, die obige Empfehlung zu verbessern, so dass Abfragen nur die „aktuellste“ Version zurückgeben

#### Bekannte Probleme

Dateidatenquellen erfordern derzeit ABIs, auch wenn ABIs nicht verwendet werden ([issue](https://github.com/graphprotocol/graph-cli/issues/961)). Umgehung ist das Hinzufügen eines beliebigen ABI.

Handler für Dateidatenquellen können nicht in Dateien sein, die `eth_call`-Vertragsbindungen importieren, und schlagen mit "unknown import: `ethereum::ethereum.call` has not been defined" ([issue](https://github.com/graphprotocol/graph-node/issues/4309)). Umgehung ist die Erstellung von Dateidatenquellen-Handlern in einer eigenen Datei.

#### Beispiele

[Crypto Coven Subgraph Migration](https://github.com/azf20/cryptocoven-api/tree/file-data-sources-refactor)

#### Referenzen

[GIP Datenquellen](https://forum.thegraph.com/t/gip-file-data-sources/2721)

## Indizierte Argumentfilter / Themen-Filter

> **Benötigt**: [SpecVersion](#specversion-releases) >= `1.2.0`

Themenfilter, auch bekannt als Filter für indizierte Argumente, sind eine leistungsstarke Funktion in Subgraphen, die es Benutzern ermöglicht, Blockchain-Ereignisse auf der Grundlage der Werte ihrer indizierten Argumente genau zu filtern.

- Diese Filter helfen dabei, bestimmte Ereignisse von Interesse aus dem riesigen Strom von Ereignissen auf der Blockchain zu isolieren, so dass Subgraphen effizienter arbeiten können, indem sie sich nur auf relevante Daten konzentrieren.

- Dies ist nützlich, um persönliche Subgraphen zu erstellen, die bestimmte Adressen und ihre Interaktionen mit verschiedenen Smart Contracts auf der Blockchain verfolgen.

### Wie Themen-Filter funktionieren

Wenn ein Smart Contract ein Ereignis auslöst, können alle Argumente, die als indiziert markiert sind, als Filter im Manifest eines Subgraphen verwendet werden. Dies ermöglicht es dem Subgraph, selektiv auf Ereignisse zu warten, die diesen indizierten Argumenten entsprechen.

- Das erste indizierte Argument des Ereignisses entspricht `topic1`, das zweite `topic2` und so weiter bis `topic3`, da die Ethereum Virtual Machine (EVM) bis zu drei indizierte Argumente pro Ereignis erlaubt.

```solidity
// SPDX-Lizenz-Identifikator: MIT
pragma solidity ^0.8.0;

contract Token {
 // Ereignisdeklaration mit indizierten Parametern für Adressen
 event Transfer(address indexed from, address indexed to, uint256 value);

 // Funktion zur Simulation der Übertragung von Token
 function transfer(address to, uint256 value) public {
 // Senden des Transfer-Ereignisses mit from, to, und value
 emit Transfer(msg.sender, to, value);
 }
}
```

In unserem Beispiel:

- Das Ereignis „Übertragung“ wird verwendet, um Transaktionen von Token zwischen Adressen zu protokollieren.
- Die Parameter „von“ und „bis“ sind indiziert, so dass Ereignisüberwacher Übertragungen mit bestimmten Adressen filtern und überwachen können.
- Die Funktion „Transfer“ ist eine einfache Darstellung einer Token-Transfer-Aktion, die bei jedem Aufruf das Ereignis „Transfer“ auslöst.

#### Konfiguration in Subgraphen

Themenfilter werden direkt in der Event-Handler-Konfiguration im Subgraph-Manifest definiert. Hier sehen Sie, wie sie konfiguriert werden:

```yaml
eventHandlers:
  - Event: SomeEvent(indexed uint256, indexed address, indexed uint256)
    handler: handleSomeEvent
    topic1: ['0xValue1', '0xValue2']
    topic2: ['0xAddress1', '0xAddress2']
    topic3: ['0xValue3']
```

In diesem Setup:

- Dabei entspricht „Thema1“ dem ersten indizierten Argument des Ereignisses, ‚Thema2‘ dem zweiten und „Thema3“ dem dritten.
- Jedes Thema kann einen oder mehrere Werte haben, und ein Ereignis wird nur verarbeitet, wenn es einem der Werte in jedem angegebenen Thema entspricht.

#### Filter-Logik

- Innerhalb eines einzelnen Themas: Die Logik funktioniert wie eine ODER-Bedingung. Das Ereignis wird verarbeitet, wenn es mit einem der aufgeführten Werte in einem bestimmten Thema übereinstimmt.
- Zwischen verschiedenen Themen: Die Logik funktioniert wie eine UND-Bedingung. Ein Ereignis muss alle angegebenen Bedingungen über verschiedene Themen hinweg erfüllen, um den zugehörigen Handler auszulösen.

#### Beispiel 1: Verfolgung von Direktüberweisungen von Adresse A nach Adresse B

```yaml
eventHandlers:
  - Event: Transfer(indizierte Adresse,indizierte Adresse,uint256)
    handler: handleDirectedTransfer
    topic1: ['0xAddressA'] # Absenderadresse
    Topic2: ['0xAddressB'] # Empfängeradresse
```

In dieser Konfiguration:

- `topic1` ist konfiguriert, `Transfer` Ereignisse zu filtern, wobei `0xAddressA` der Absender ist.
- Thema2„ ist so konfiguriert, dass Ereignisse der Kategorie ‚Übertragung‘ gefiltert werden, bei denen “0xAdresseB" der Empfänger ist.
- Der Subgraph indiziert nur Transaktionen, die direkt von „0xAdresseA“ nach „0xAdresseB“ erfolgen.

#### Beispiel 2: Verfolgung von Transaktionen in beiden Richtungen zwischen zwei oder mehr Adressen

```yaml
eventHandlers:
 - event: Transfer(indexed address,indexed address,uint256)
 handler: handleTransferToOrFrom
 topic1: ['0xAddressA', '0xAddressB', '0xAddressC'] # Absenderadresse
 topic2: ['0xAddressB', '0xAddressC'] # Empfängeradresse
```

In dieser Konfiguration:

- Thema1„ ist so konfiguriert, dass er “Transfer„-Ereignisse filtert, bei denen “0xAdresseA„, ‚0xAdresseB‘, “0xAdresseC" der Absender ist.
- Thema2„ ist so konfiguriert, dass es “Transfer„-Ereignisse filtert, bei denen ‚0xAdresseB‘ und “0xAdresseC" der Empfänger ist.
- Der Subgraph indiziert Transaktionen, die in beiden Richtungen zwischen mehreren Adressen stattfinden, und ermöglicht so eine umfassende Überwachung von Interaktionen, die alle Adressen betreffen.

## Deklariert eth_call

> Hinweis: Dies ist eine experimentelle Funktion, die derzeit noch nicht in einer stabilen Graph Node-Version verfügbar ist. Sie können sie nur in Subgraph Studio oder Ihrem selbst gehosteten Knoten verwenden.

Deklarative `eth_calls` sind eine wertvolle Funktion von Subgraph, die es erlaubt, `eth_calls` im Voraus auszuführen, so dass `graph-node` sie parallel ausführen kann.

Diese Funktion hat die folgenden Aufgaben:

- Erhebliche Verbesserung der Leistung beim Abrufen von Daten aus der Ethereum-Blockchain durch Reduzierung der Gesamtzeit für mehrere Aufrufe und Optimierung der Gesamteffizienz des Subgraphen.
- Ermöglicht einen schnelleren Datenabruf, was zu schnelleren Abfrageantworten und einer besseren Benutzerfreundlichkeit führt.
- Reduziert die Wartezeiten für Anwendungen, die Daten aus mehreren Ethereum-Aufrufen aggregieren müssen, und macht den Datenabrufprozess effizienter.

### Schlüsselkonzepte

- Deklarative `eth_calls`: Ethereum-Aufrufe, die so definiert sind, dass sie parallel und nicht sequentiell ausgeführt werden.
- Parallele Ausführung: Anstatt auf das Ende eines Aufrufs zu warten, bevor der nächste gestartet wird, können mehrere Aufrufe gleichzeitig gestartet werden.
- Zeiteffizienz: Die Gesamtzeit, die für alle Anrufe benötigt wird, ändert sich von der Summe der einzelnen Anrufzeiten (sequentiell) zur Zeit des längsten Anrufs (parallel).

#### Szenario ohne deklarative `eth_calls`

Stellen Sie sich vor, Sie haben einen Subgraph, der drei Ethereum-Aufrufe tätigen muss, um Daten über die Transaktionen, den Kontostand und den Token-Besitz eines Nutzers abzurufen.

Traditionell können diese Anrufe nacheinander erfolgen:

1. Anruf 1 (Transaktionen): dauert 3 Sekunden
2. Aufruf 2 (Balance): Dauert 2 Sekunden
3. Aufruf 3 (Besitz von Token): Dauert 4 Sekunden

Gesamte benötigte Zeit = 3 + 2 + 4 = 9 Sekunden

#### Szenario mit deklarativen `eth_calls`

Mit dieser Funktion können Sie erklären, dass diese Aufrufe parallel ausgeführt werden sollen:

1. Anruf 1 (Transaktionen): dauert 3 Sekunden
2. Aufruf 2 (Balance): Dauert 2 Sekunden
3. Aufruf 3 (Besitz von Token): Dauert 4 Sekunden

Da diese Aufrufe parallel ausgeführt werden, entspricht die Gesamtzeit der Zeit, die der längste Aufruf benötigt.

Insgesamt benötigte Zeit = max (3, 2, 4) = 4 Sekunden

#### So funktioniert's

1. Deklarative Definition: Im Subgraph-Manifest deklarieren Sie die Ethereum-Aufrufe in einer Weise, die angibt, dass sie parallel ausgeführt werden können.
2. Parallele Ausführungsmaschine: Die Ausführungsmaschine des Graph Node erkennt diese Deklarationen und führt die Aufrufe gleichzeitig aus.
3. Ergebnis-Aggregation: Sobald alle Aufrufe abgeschlossen sind, werden die Ergebnisse aggregiert und vom Subgraphen für die weitere Verarbeitung verwendet.

#### Beispielkonfiguration im Subgraph Manifest

Deklarierte `eth_calls` können auf die `event.address` des zugrunde liegenden Ereignisses sowie auf alle `event.params` zugreifen.

subgraph.yaml„ unter Verwendung von “event.address":

```yaml
eventHandlers:
event: Swap(indexed address,indexed address,int256,int160,uint128,int24)
handler: handleSwap
calls:
  global0X128: Pool[event.address].feeGrowthGlobal0X128()
  global1X128: Pool[event.address].feeGrowthGlobal1X128()
```

Details für das obige Beispiel:

- global0X128„ ist der angegebene “eth_call".
- Der Text (`global0X128`) ist die Bezeichnung für diesen `eth_call`, die bei der Fehlerprotokollierung verwendet wird.
- Der Text (`Pool[event.address].feeGrowthGlobal0X128()`) ist der eigentliche `eth_call`, der in Form von `Contract[address].function(arguments)` ausgeführt wird.
- Die „Adresse“ und die „Argumente“ können durch Variablen ersetzt werden, die bei der Ausführung des Handlers verfügbar sein werden.

subgraph.yaml„ unter Verwendung von “event.params

```yaml
Aufrufe:
  - ERC20DecimalsToken0: ERC20[event.params.token0].decimals()
```

### Grafting auf bestehende Subgraphen

> **Hinweis:** Es wird nicht empfohlen, beim ersten Upgrade auf The Graph Network das Grafting zu verwenden. Erfahren Sie mehr [hier](/subgraphs/cookbook/grafting/#important-note-on-grafting-when-upgrading-to-the-network).

Wenn ein Subgraph zum ersten Mal eingesetzt wird, beginnt er mit der Indizierung von Ereignissen am Entstehungsblock der entsprechenden Kette (oder am `startBlock`, der mit jeder Datenquelle definiert ist). Unter bestimmten Umständen ist es von Vorteil, die Daten eines bestehenden Subgraphen wiederzuverwenden und die Indizierung an einem viel späteren Block zu beginnen. Diese Art der Indizierung wird _Grafting_ genannt. Grafting ist z.B. während der Entwicklung nützlich, um einfache Fehler in den Mappings schnell zu beheben oder um einen bestehenden Subgraph nach einem Fehler vorübergehend wieder zum Laufen zu bringen.

Ein Subgraph wird auf einen Basis-Subgraph gepfropft, wenn das Subgraph-Manifest in `subgraph.yaml` einen `graft`-Block auf der obersten Ebene enthält:

```yaml
Beschreibung: ...
graft:
  base: Qm ... # Subgraph ID des Basis-Subgraphen
  block: 7345624 # Blocknummer
```

Wenn ein Subgraph, dessen Manifest einen „graft“-Block enthält, bereitgestellt wird, kopiert Graph Node die Daten des ‚Basis‘-Subgraphen bis einschließlich des angegebenen „Blocks“ und fährt dann mit der Indizierung des neuen Subgraphen ab diesem Block fort. Der Basis-Subgraph muss auf der Ziel-Graph-Node-Instanz existieren und mindestens bis zum angegebenen Block indexiert sein. Aufgrund dieser Einschränkung sollte Grafting nur während der Entwicklung oder in Notfällen verwendet werden, um die Erstellung eines äquivalenten, nicht gepfropften Subgraphen zu beschleunigen.

Da beim Grafting die Basisdaten kopiert und nicht indiziert werden, ist es viel schneller, den Subgraphen auf den gewünschten Block zu bringen, als wenn er von Grund auf neu indiziert wird, obwohl die anfängliche Datenkopie bei sehr großen Subgraphen immer noch mehrere Stunden dauern kann. Während der Initialisierung des gepfropften Subgraphen protokolliert der Graph Node Informationen über die bereits kopierten Entitätstypen.

Der aufgepfropfte Subgrafen kann ein GraphQL-Schema verwenden, das nicht identisch mit dem des Basis-Subgrafen ist, sondern lediglich mit diesem kompatibel ist. Es muss ein eigenständig gültiges Subgrafen-Schema sein, darf aber auf folgende Weise vom Schema des Basis-Subgrafen abweichen:

- Es fügt Entitätstypen hinzu oder entfernt sie
- Es entfernt Attribute von Entitätstypen
- Es fügt Entitätstypen nullfähige Attribute hinzu
- Es wandelt Nicht-Nullable-Attribute in Nullable-Attribute um
- Es fügt Aufzählungen Werte hinzu
- Es fügt Interface hinzu oder entfernt sie
- Sie ändert sich je nachdem, für welche Art von Elementen das Interface implementiert ist

> **[Feature Management](#experimental-features):** `grafting` muss unter `features` im Subgraph-Manifest deklariert werden.
