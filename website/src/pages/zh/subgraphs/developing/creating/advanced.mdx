---
title: 高级子图功能
---

## 概述

添加并实现高级子图功能，以增强子图的构建。

从 ```specVersion ``0.0.4``` 开始，子图特征必须使用它们的 `camelCase` 名称，在清单文件顶层的 `features` 部分中显式声明，如下表所列：

| 功能                                              | 名字               |
| ----------------------------------------------- | ---------------- |
| [非致命错误](#non-fatal-errors)                      | `nonFatalErrors` |
| [全文搜索](#defining-fulltext-search-fields)        | `fullTextSearch` |
| [嫁接](#grafting-onto-existing-subgraphs)         | `grafting`       |

例如，如果子图使用 **Full-Text Search** 和 **Non-fatal Errors** 功能，则清单中的 `features` 字段应为：

```yaml
specVersion: 1.3.0
description: Gravatar for Ethereum
features:
  - fullTextSearch
  - nonFatalErrors
dataSources: ……
```

> 请注意，在子图部署期间使用未声明的特性会导致**验证错误**，但如果声明了特性未使用，则不会出现错误。

## 时间序列和聚合

先决条件：

- 子图规范版本必须≥1.1.0。

时间序列和聚合使您的子图能够跟踪每日平均价格、每小时总转账等统计数据。

此功能引入了两种新型子图实体。时间序列实体用时间戳记录数据点。聚合实体每小时或每天对时间序列数据点执行预先声明的计算，然后存储结果，以便通过GraphQL轻松访问。

### 示例模式

```graphql
type Data @entity(timeseries: true) {
  id: Int8!
  timestamp: Timestamp!
  price: BigDecimal!
}

type Stats @aggregation(intervals: ["hour", "day"], source: "Data") {
  id: Int8!
  timestamp: Timestamp!
  sum: BigDecimal! @aggregate(fn: "sum", arg: "price")
}
```

### 如何实现时间序列和聚合

时间序列实体在GraphQL模式中用`@entity（Timeseries：true）`定义。每个时间序列实体必须：

- 具有int8类型的唯一ID
- 具有timestamp类型的时间戳
- include data that will be used for calculation by aggregation entities.

These Timeseries entities can be saved in regular trigger handlers, and act as the “raw data” for the aggregation entities.

Aggregation entities are defined with `@aggregation` in the GraphQL schema. Every aggregation entity defines the source from which it will gather data (which must be a timeseries entity), sets the intervals (e.g., hour, day), and specifies the aggregation function it will use (e.g., sum, count, min, max, first, last).

Aggregation entities are automatically calculated on the basis of the specified source at the end of the required interval.

#### Available Aggregation Intervals

- `hour`: sets the timeseries period every hour, on the hour.
- `day`: sets the timeseries period every day, starting and ending at 00:00.

#### Available Aggregation Functions

- `sum`: Total of all values.
- `count`: Number of values.
- `min`: Minimum value.
- `max`: Maximum value.
- `first`: First value in the period.
- `last`: Last value in the period.

#### Example Aggregations Query

```graphql
{
  stats(interval: "hour", where: { timestamp_gt: 1704085200 }) {
    id
    timestamp
    sum
  }
}
```

[Read more](https://github.com/graphprotocol/graph-node/blob/master/docs/aggregations.md) about Timeseries and Aggregations.

## 非致命错误

Indexing errors on already synced Subgraphs will, by default, cause the Subgraph to fail and stop syncing. Subgraphs can alternatively be configured to continue syncing in the presence of errors, by ignoring the changes made by the handler which provoked the error. This gives Subgraph authors time to correct their Subgraphs while queries continue to be served against the latest block, though the results might be inconsistent due to the bug that caused the error. Note that some errors are still always fatal. To be non-fatal, the error must be known to be deterministic.

> **Note:** The Graph Network does not yet support non-fatal errors, and developers should not deploy Subgraphs using that functionality to the network via the Studio.

Enabling non-fatal errors requires setting the following feature flag on the Subgraph manifest:

```yaml
specVersion: 1.3.0
description: Gravatar for Ethereum
features:
    - nonFatalErrors
    ...
```

The query must also opt-in to querying data with potential inconsistencies through the `subgraphError` argument. It is also recommended to query `_meta` to check if the Subgraph has skipped over errors, as in the example:

```graphql
foos(first: 100, subgraphError: allow) {
  id
}

_meta {
  hasIndexingErrors
}
```

If the Subgraph encounters an error, that query will return both the data and a graphql error with the message `"indexing_error"`, as in this example response:

```graphql
"data": {
    "foos": [
        {
          "id": "0xdead"
        }
    ],
    "_meta": {
        "hasIndexingErrors": true
    }
},
"errors": [
    {
        "message": "indexing_error"
    }
]
```

## IPFS/Arweave File Data Sources

File data sources are a new Subgraph functionality for accessing off-chain data during indexing in a robust, extendable way. File data sources support fetching files from IPFS and from Arweave.

> 这也为链外数据的确定性索引以及引入任意HTTP源数据奠定了基础。

### 概述

Rather than fetching files "in line" during handler execution, this introduces templates which can be spawned as new data sources for a given file identifier. These new data sources fetch the files, retrying if they are unsuccessful, running a dedicated handler when the file is found.

This is similar to the [existing data source templates](/developing/creating-a-subgraph/#data-source-templates), which are used to dynamically create new chain-based data sources.

> This replaces the existing `ipfs.cat` API

### 升级指南

#### Update `graph-ts` and `graph-cli`

File data sources requires graph-ts >=0.29.0 and graph-cli >=0.33.1

#### 添加新的实体类型，当找到文件时将更新该类型

文件数据源不能访问或更新基于链的实体，但必须更新特定于文件的实体。

这可能意味着将现有实体中的字段拆分为单独的实体，并链接在一起。

原始合并实体：

```graphql
type Token @entity {
  id: ID!
  tokenID: BigInt!
  tokenURI: String!
  externalURL: String!
  ipfsURI: String!
  image: String!
  name: String!
  description: String!
  type: String!
  updatedAtTimestamp: BigInt
  owner: User!
}
```

新拆分实体：

```graphql
type Token @entity {
  id: ID!
  tokenID: BigInt!
  tokenURI: String!
  ipfsURI: TokenMetadata
  updatedAtTimestamp: BigInt
  owner: String!
}

type TokenMetadata @entity {
  id: ID!
  image: String!
  externalURL: String!
  name: String!
  description: String!
}
```

如果母实体与生成的文件数据源实体之间的关系为1:1，则最简单的模式是通过使用IPFS CID作为查找将母实体链接到生成的文件实体。如果您在建模新的基于文件的实体时遇到困难，请联系Discord！

> You can use [nested filters](/subgraphs/querying/graphql-api/#example-for-nested-entity-filtering) to filter parent entities on the basis of these nested entities.

#### Add a new templated data source with `kind: file/ipfs` or `kind: file/arweave`

这是在识别出感兴趣的文件时生成的数据源。

```yaml
templates:
  - name: TokenMetadata
    kind: file/ipfs
    mapping:
      apiVersion: 0.0.9
      language: wasm/assemblyscript
      file: ./src/mapping.ts
      handler: handleMetadata
      entities:
        - TokenMetadata
      abis:
        - name: Token
          file: ./abis/Token.json
```

> Currently `abis` are required, though it is not possible to call contracts from within file data sources

The file data source must specifically mention all the entity types which it will interact with under `entities`. See [limitations](#limitations) for more details.

#### 创建新处理程序以处理文件

This handler should accept one `Bytes` parameter, which will be the contents of the file, when it is found, which can then be processed. This will often be a JSON file, which can be processed with `graph-ts` helpers ([documentation](/subgraphs/developing/creating/graph-ts/api/#json-api)).

The CID of the file as a readable string can be accessed via the `dataSource` as follows:

```typescript
const cid = dataSource.stringParam()
```

示例处理程序：

```typescript
import { json, Bytes, dataSource } from '@graphprotocol/graph-ts'
import { TokenMetadata } from '../generated/schema'

export function handleMetadata(content: Bytes): void {
  let tokenMetadata = new TokenMetadata(dataSource.stringParam())
  const value = json.fromBytes(content).toObject()
  if (value) {
    const image = value.get('image')
    const name = value.get('name')
    const description = value.get('description')
    const externalURL = value.get('external_url')

    if (name && image && description && externalURL) {
      tokenMetadata.name = name.toString()
      tokenMetadata.image = image.toString()
      tokenMetadata.externalURL = externalURL.toString()
      tokenMetadata.description = description.toString()
    }

    tokenMetadata.save()
  }
}
```

#### 需要时生成文件数据源

现在，您可以在执行基于链的处理程序期间创建文件数据源：

- Import the template from the auto-generated `templates`
- call `TemplateName.create(cid: string)` from within a mapping, where the cid is a valid content identifier for IPFS or Arweave

For IPFS, Graph Node supports [v0 and v1 content identifiers](https://docs.ipfs.tech/concepts/content-addressing/), and content identifiers with directories (e.g. `bafyreighykzv2we26wfrbzkcdw37sbrby4upq7ae3aqobbq7i4er3tnxci/metadata.json`).

For Arweave, as of version 0.33.0 Graph Node can fetch files stored on Arweave based on their [transaction ID](https://docs.arweave.org/developers/arweave-node-server/http-api#transactions) from an Arweave gateway ([example file](https://bdxujjl5ev5eerd5ouhhs6o4kjrs4g6hqstzlci5pf6vhxezkgaa.arweave.net/CO9EpX0lekJEfXUOeXncUmMuG8eEp5WJHXl9U9yZUYA)). Arweave supports transactions uploaded via Irys (previously Bundlr), and Graph Node can also fetch files based on [Irys manifests](https://docs.irys.xyz/overview/gateways#indexing).

例子:

```typescript
import { TokenMetadata as TokenMetadataTemplate } from '../generated/templates'

const ipfshash = 'QmaXzZhcYnsisuue5WRdQDH6FDvqkLQX1NckLqBYeYYEfm'
//This example code is for a Crypto coven Subgraph. The above ipfs hash is a directory with token metadata for all crypto coven NFTs.

export function handleTransfer(event: TransferEvent): void {
  let token = Token.load(event.params.tokenId.toString())
  if (!token) {
    token = new Token(event.params.tokenId.toString())
    token.tokenID = event.params.tokenId

    token.tokenURI = '/' + event.params.tokenId.toString() + '.json'
    const tokenIpfsHash = ipfshash + token.tokenURI
    //This creates a path to the metadata for a single Crypto coven NFT. It concats the directory with "/" + filename + ".json"

    token.ipfsURI = tokenIpfsHash

    TokenMetadataTemplate.create(tokenIpfsHash)
  }

  token.updatedAtTimestamp = event.block.timestamp
  token.owner = event.params.to.toHexString()
  token.save()
}
```

这将创建一个新的文件数据源，该数据源将轮询Graph Node配置的IPFS或Arweave端点，如果未找到文件，则进行重试。当找到文件时，文件数据源处理程序将被执行。

This example is using the CID as the lookup between the parent `Token` entity and the resulting `TokenMetadata` entity.

> Previously, this is the point at which a Subgraph developer would have called `ipfs.cat(CID)` to fetch the file

祝贺您，您正在使用文件数据源！

#### Deploying your Subgraphs

You can now `build` and `deploy` your Subgraph to any Graph Node >=v0.30.0-rc.0.

#### 限制

File data source handlers and entities are isolated from other Subgraph entities, ensuring that they are deterministic when executed, and ensuring no contamination of chain-based data sources. To be specific:

- 文件数据源创建的实体是不可变的，不能更新
- 文件数据源处理程序无法访问其他文件数据源中的实体
- 基于链的处理程序无法访问与文件数据源关联的实体

> While this constraint should not be problematic for most use-cases, it may introduce complexity for some. Please get in touch via Discord if you are having issues modelling your file-based data in a Subgraph!

此外，不可能从文件数据源创建数据源，无论是线上数据源还是其他文件数据源。这项限制将来可能会取消。

#### 最佳实践

如果要将 NFT 元数据链接到相应的代币，请使用元数据的 IPFS hash从代币实体引用元数据实体。使用 IPFS hash作为 ID 保存元数据实体。

在创建文件数据源时，您可以使用[DataSource上下文](/subgraphs/developing/creating/graph-ts/api/#entity-and-datasourcecontext)传递额外的信息，这些信息将可供文件数据源处理程序使用。

如果您有多次刷新的实体，请使用 IPFS  一的基于文件的实体。实体 ID，并使用基于链的实体中的派生字段引用它们。

> 我们正在努力改进上述建议，因此查询只返回“最新”版本。

#### 已知问题

文件数据源目前需要ABI，即使不使用ABI（[问题](https://github.com/graphprotocol/graph-cli/issues/961))）。解决方法是添加任何ABI。

文件数据源的处理程序不能在导入 `eth _ call` 契约绑定的文件中，如果“未知导入: `etherum:: etherum.call` 尚未定义”([问题](https://github.com/graphprotocol/graph-node/issues/4309)) 则失败。解决办法是在专用文件中创建文件数据源处理程序。

#### 例子

[Crypto-Coven子图迁移](https://github.com/azf20/cryptocoven-api/tree/file-data-sources-refactor)

#### 参考

[GIP文件数据源](https://forum.thegraph.com/t/gip-file-data-sources/2721)

## 索引参数过滤器 / 主题过滤器

> **要求**: [规范版本](#specversion-releases) >= `1.2.0`

主题过滤器，也称为索引参数过滤器，是子图中的一个强大功能，允许用户根据其索引参数的值精确过滤区块链事件。

- 这些过滤器有助于将感兴趣的特定事件与区块链上的大量事件流隔离开来，通过只关注相关数据，使子图能够更有效地运行。

- 这对于创建跟踪特定地址及其与区块链上各种智能合约交互的个人子图非常有用。

### 主题过滤器的工作原理

当智能合约发出事件时，任何标记为索引的参数都可以用作子图清单中的过滤器。这允许子图有选择地监听与这些索引参数匹配的事件。

- 事件的第一个索引参数对应`topic1`，第二个对应`topic2`，以此类推，直到`topic3`，因为以太坊虚拟机（EVM）允许每个事件最多三个索引参数。

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

contract Token {
    // Event declaration with indexed parameters for addresses
    event Transfer(address indexed from, address indexed to, uint256 value);

    // Function to simulate transferring tokens
    function transfer(address to, uint256 value) public {
        // Emitting the Transfer event with from, to, and value
        emit Transfer(msg.sender, to, value);
    }
}
```

在这个例子中：

- `Transfer`事件用于记录地址之间的代币交易。
- `from`和`to`参数被索引，允许事件侦听器过滤和监视涉及特定地址的传输。
- `传输`函数是代币传输操作的简单表示，每当调用时都会发出传输事件。

#### 子图中的配置

主题过滤器直接在子图清单的事件处理程序配置中定义。以下是它们的配置方式：

```yaml
eventHandlers:
  - event: SomeEvent(indexed uint256, indexed address, indexed uint256)
    handler: handleSomeEvent
    topic1: ['0xValue1', '0xValue2']
    topic2: ['0xAddress1', '0xAddress2']
    topic3: ['0xValue3']
```

在此设置中：

- `topic1`对应于事件的第一个索引参数，`topic2`对应于第二个，`topic3`对应于第三个。
- 每个主题可以有一个或多个值，只有当事件与每个指定主题中的一个值匹配时，才会处理该事件。

#### 过滤器逻辑

- 在单个主题中：逻辑功能充当OR条件。如果事件与给定主题中列出的任何一个值匹配，则将对其进行处理。
- 不同主题之间：逻辑作为AND条件发挥作用。事件必须满足不同主题的所有指定条件，才能触发关联的处理程序。

#### 示例1：跟踪从地址A到地址B的直接传输

```yaml
eventHandlers:
  - event: Transfer(indexed address,indexed address,uint256)
    handler: handleDirectedTransfer
    topic1: ['0xAddressA'] # Sender Address
    topic2: ['0xAddressB'] # Receiver Address
```

在此配置中：

- `topic1`被配置为过滤`0xAddressA`为发送方的`传输`事件。
- `topic2`被配置为过滤`0xAddressB`为接收方的`传输`事件。
- 子图将仅索引直接从`0xAddressA` 到 `0xAddressB`发生的交易。

#### 示例2：跟踪两个或多个地址之间的双向交易

```yaml
eventHandlers:
  - event: Transfer(indexed address,indexed address,uint256)
    handler: handleTransferToOrFrom
    topic1: ['0xAddressA', '0xAddressB', '0xAddressC'] # Sender Address
    topic2: ['0xAddressB', '0xAddressC'] # Receiver Address
```

在此配置中：

- `topic1`被配置为过滤`0xAddressA`、`0xAddressB`、`0xAddressC`为发送方的`传输`事件。
- `topic2`被配置为过滤`0xAddressB`和`0xAddressC`为接收方的`传输`事件。
- 子图将对多个地址之间双向发生的交易进行索引，从而全面监控涉及所有地址的交互。

## 已声明eth_call

> 注意：这是一个实验性功能，目前尚未在稳定的Graph Node版本中提供。您只能在Subgraph Studio或自托管节点中使用它。

声明性`eth_calls`是一个有价值的子图特性，它允许`eth_call`提前执行，使`graph-node`能够并行执行它们。

此功能执行以下操作：

- 通过减少多次调用的总时间和优化子图的整体效率，显著提高了从以太坊区块链获取数据的性能。
- 允许更快的数据获取，从而获得更快的查询响应和更好的用户体验。
- 减少需要聚合来自多个以太坊调用的数据的应用程序的等待时间，使数据检索过程更加高效。

### 关键概念

- 声明性`eth_calls`：定义为并行执行而非顺序执行的以太坊调用。
- Parallel Execution: Instead of waiting for one call to finish before starting the next, multiple calls can be initiated simultaneously.
- Time Efficiency: The total time taken for all the calls changes from the sum of the individual call times (sequential) to the time taken by the longest call (parallel).

#### Scenario without Declarative `eth_calls`

Imagine you have a Subgraph that needs to make three Ethereum calls to fetch data about a user's transactions, balance, and token holdings.

Traditionally, these calls might be made sequentially:

1. 调用 1 (交易): 需要3 秒
2. Call 2 (Balance): Takes 2 seconds
3. 调用 3 (代币持有): 需要4 秒

Total time taken = 3 + 2 + 4 = 9 seconds

#### Scenario with Declarative `eth_calls`

With this feature, you can declare these calls to be executed in parallel:

1. 调用 1 (交易): 需要3 秒
2. Call 2 (Balance): Takes 2 seconds
3. 调用 3 (代币持有): 需要4 秒

Since these calls are executed in parallel, the total time taken is equal to the time taken by the longest call.

Total time taken = max (3, 2, 4) = 4 seconds

#### How it Works

1. Declarative Definition: In the Subgraph manifest, you declare the Ethereum calls in a way that indicates they can be executed in parallel.
2. Parallel Execution Engine: The Graph Node's execution engine recognizes these declarations and runs the calls simultaneously.
3. Result Aggregation: Once all calls are complete, the results are aggregated and used by the Subgraph for further processing.

#### Example Configuration in Subgraph Manifest

Declared `eth_calls` can access the `event.address` of the underlying event as well as all the `event.params`.

`subgraph.yaml` using `event.address`:

```yaml
eventHandlers:
event: Swap(indexed address,indexed address,int256,int256,uint160,uint128,int24)
handler: handleSwap
calls:
  global0X128: Pool[event.address].feeGrowthGlobal0X128()
  global1X128: Pool[event.address].feeGrowthGlobal1X128()
```

Details for the example above:

- `global0X128` is the declared `eth_call`.
- The text (`global0X128`) is the label for this `eth_call` which is used when logging errors.
- The text (`Pool[event.address].feeGrowthGlobal0X128()`) is the actual `eth_call` that will be executed, which is in the form of `Contract[address].function(arguments)`
- The `address` and `arguments` can be replaced with variables that will be available when the handler is executed.

`subgraph.yaml` using `event.params`

```yaml
calls:
  - ERC20DecimalsToken0: ERC20[event.params.token0].decimals()
```

### 嫁接到现有子图

> **Note:** it is not recommended to use grafting when initially upgrading to The Graph Network. Learn more [here](/subgraphs/cookbook/grafting/#important-note-on-grafting-when-upgrading-to-the-network).

When a Subgraph is first deployed, it starts indexing events at the genesis block of the corresponding chain (or at the `startBlock` defined with each data source) In some circumstances; it is beneficial to reuse the data from an existing Subgraph and start indexing at a much later block. This mode of indexing is called _Grafting_. Grafting is, for example, useful during development to get past simple errors in the mappings quickly or to temporarily get an existing Subgraph working again after it has failed.

A Subgraph is grafted onto a base Subgraph when the Subgraph manifest in `subgraph.yaml` contains a `graft` block at the top-level:

```yaml
description: ...
graft:
  base: Qm... # Subgraph ID of base Subgraph
  block: 7345624 # Block number
```

When a Subgraph whose manifest contains a `graft` block is deployed, Graph Node will copy the data of the `base` Subgraph up to and including the given `block` and then continue indexing the new Subgraph from that block on. The base Subgraph must exist on the target Graph Node instance and must have indexed up to at least the given block. Because of this restriction, grafting should only be used during development or during an emergency to speed up producing an equivalent non-grafted Subgraph.

Because grafting copies rather than indexes base data, it is much quicker to get the Subgraph to the desired block than indexing from scratch, though the initial data copy can still take several hours for very large Subgraphs. While the grafted Subgraph is being initialized, the Graph Node will log information about the entity types that have already been copied.

The grafted Subgraph can use a GraphQL schema that is not identical to the one of the base Subgraph, but merely compatible with it. It has to be a valid Subgraph schema in its own right, but may deviate from the base Subgraph's schema in the following ways:

- 添加或删除实体类型
- 从实体类型中删除属性
- 将可为空的属性添加到实体类型
- 将不可为空的属性转换为可空的属性
- 将值添加到枚举类型中
- 添加或删除接口
- 改变了实现接口的实体类型

> **[Feature Management](#experimental-features):** `grafting` must be declared under `features` in the Subgraph manifest.
