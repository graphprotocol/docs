---
title: Uzel provozního graf
---

Graf Uzel je komponenta, která indexuje podgrafy a zpřístupňuje výsledná data k dotazování prostřednictvím rozhraní GraphQL API. Jako taková je ústředním prvkem zásobníku indexeru a její správná činnost je pro úspěšný provoz indexeru klíčová.

Tato část poskytuje kontextový přehled o uzlu Graf a o některých pokročilejších možnostech, které jsou indexátorům k dispozici. Podrobnou dokumentaci a pokyny najdete v úložišti [Graf Uzel](https://github.com/graphprotocol/graph-node).

## Uzel grafu

[Graf Uzel](https://github.com/graphprotocol/graph-node) je referenční implementace pro indexování podgrafů v síti Graf, připojení ke klientům blockchainu, indexování podgrafů a zpřístupnění indexovaných dat k dotazování.

Graf Uzel (a celý indexer stack) lze provozovat na holém železe nebo v cloudovém prostředí. Tato flexibilita centrální indexovací komponenty je klíčová pro robustnost protokolu v Graf Protocol. Stejně tak může být Graf Uzel [postaven ze zdrojového kódu](https://github.com/graphprotocol/graph-node), nebo mohou indexátory používat jeden z [poskytovaných obrazů Docker](https://hub.docker.com/r/graphprotocol/graph-node).

### Databáze PostgreSQL

Hlavní úložiště pro uzel Graf Uzel, kde jsou uložena data podgrafů, metadata o podgraf a síťová data týkající se podgrafů, jako je bloková cache a cache eth_call.

### Síťoví klienti

Aby mohl uzel Graph Node indexovat síť, potřebuje přístup k síťovému klientovi prostřednictvím rozhraní API JSON-RPC kompatibilního s EVM. Toto RPC se může připojit k jedinému klientovi nebo může jít o složitější nastavení, které vyrovnává zátěž mezi více klienty.

Zatímco některé podgrafy mohou vyžadovat pouze plný uzel, některé mohou mít indexovací funkce, které vyžadují další funkce RPC. Konkrétně podgrafy, které v rámci indexování provádějí `eth_calls`, budou vyžadovat archivní uzel, který podporuje [EIP-1898](https://eips.ethereum.org/EIPS/eip-1898), a podgrafy s `callHandlers` nebo `blockHandlers` s filtrem `call` vyžadují podporu `trace_filter` ([viz dokumentace modulu trace zde](https://openethereum.github.io/JSONRPC-trace-module)).

**Network Firehoses** - Firehose je služba gRPC poskytující uspořádaný, ale vidlicově orientovaný proud bloků, vyvinutá hlavními vývojáři Grafu pro lepší podporu výkonného indexování v měřítku. V současné době to není požadavek na indexátor, ale doporučujeme indexátorům, aby se s touto technologií seznámili ještě před plnou podporou sítě. Více informací o Firehose [zde](https://firehose.streamingfast.io/).

### IPFS uzly

Metadata nasazení podgrafů jsou uložena v síti IPFS. Uzel Graf přistupuje během nasazení podgrafu především k uzlu IPFS, aby načetl manifest podgrafu a všechny propojené soubory. Síťové indexery nemusí hostit vlastní uzel IPFS. Uzel IPFS pro síť je hostován na adrese https://ipfs.network.thegraph.com.

### Metrický server Prometheus

Aby bylo možné monitorovat a podávat zprávy, může uzel Graf volitelně zaznamenávat metriky na metrický server Prometheus.

### Začínáme od zdroje

#### Instalace předpokladů

- **Rust**

- **PostgreSQL**

- **IPFS**

- **Další požadavky pro uživatele Ubuntu** - Pro spuštění Uzel Graf v Ubuntu může být potřeba několik dalších balíčků.

```sh
sudo apt-get install -y clang libpg-dev libssl-dev pkg-config
```

#### Nastavení

1. Spuštění databázového serveru PostgreSQL

```sh
initdb -D .postgres
pg_ctl -D .postgres -l logfile start
createdb graph-node
```

2. Klonujte repozitář [Uzel Graf](https://github.com/graphprotocol/graph-node) a sestavte zdrojový kód spuštěním příkazu `cargo build`

3. Nyní, když jsou všechny závislosti nastaveny, spusťte uzel Graf:

```sh
cargo run -p graph-node --release -- \
  --postgres-url postgresql://[USERNAME]:[PASSWORD]@localhost:5432/graph-node \
  --ethereum-rpc [NETWORK_NAME]:[URL] \
  --ipfs https://ipfs.network.thegraph.com
```

### Začínáme s Kubernetes

Kompletní příklad konfigurace Kubernetes naleznete v úložišti [indexer](https://github.com/graphprotocol/indexer/tree/main/k8s).

### Ports

Když je Graf Uzel spuštěn, zpřístupňuje následující ports:

| Port | Účel | Trasy | CLI Argument | Proměnná prostředí |
| --- | --- | --- | --- | --- |
| 8000 | GraphQL HTTP server<br />(pro dotazy podgrafy) | /subgraphs/id/...<br />/subgraphs/name/.../... | --http-port | - |
| 8001 | GraphQL WS<br />(pro odběry podgrafů) | /subgraphs/id/...<br />/subgraphs/name/.../... | --ws-port | - |
| 8020 | JSON-RPC<br />(pro správu nasazení) | / | --admin-port | - |
| 8030 | Stav indexování podgrafů API | /graphql | --index-node-port | - |
| 8040 | Metriky Prometheus | /metrics | --metrics-port | - |

> **Důležité**: Dávejte pozor na veřejné vystavování portů - **administrační porty** by měly být uzamčeny. To se týká i koncového bodu JSON-RPC uzlu Graf.

## Pokročilá konfigurace uzlu Graf

V nejjednodušším případě lze Graf Uzel provozovat s jednou instancí Graf Uzel, jednou databází PostgreSQL, uzlem IPFS a síťovými klienty podle potřeby indexovaných podgrafů.

Toto nastavení lze horizontálně škálovat přidáním více graf uzlů a více databází pro podporu těchto graf uzlů. Pokročilí uživatelé mohou chtít využít některé možnosti horizontálního škálování Graf Uzel a také některé pokročilejší možnosti konfigurace prostřednictvím souboru `config.toml` a proměnných prostředí Graph Node.

### `config.toml`

Konfigurační soubor [TOML](https://toml.io/en/) lze použít k nastavení složitějších konfigurací, než jaké jsou dostupné v CLI. Umístění souboru se předává pomocí přepínače příkazového řádku --config.

> Při použití konfiguračního souboru není možné použít volby --postgres-url, --postgres-secondary-hosts a --postgres-host-weights.

Lze zadat minimální soubor `config.toml`; následující soubor je ekvivalentní použití volby příkazového řádku --postgres-url:

```toml
[store]
[store.primary]
connection="<.. postgres-url argument ..>"
[deployment]
[[deployment.rule]]
indexers = [ "<.. list of all indexing nodes ..>" ]
```

Úplnou dokumentaci `config.toml` lze nalézt v [dokumentech Graf Uzel](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md).

#### Více uzlů graf

Indexování Graf Uzel lze horizontálně škálovat, přičemž lze spustit více instancí Graf Uzel a rozdělit indexování a dotazování mezi různé uzly. To lze provést jednoduše spuštěním uzlu Graf nakonfigurovaného s jiným `id uzlu` při spuštění (např. v souboru Docker Compose), který lze poté použít v souboru `config.toml` k určení [dedikovaných uzlů pro dotazování](#dedicated-query-nodes), [blokových ingestorů](#dedicated-block-ingestor) a rozdělení podgrafů mezi uzly pomocí [pravidel nasazení](#deployment-rules).

> Všimněte si, že více graf uzlů lze nakonfigurovat tak, aby používaly stejnou databázi, kterou lze horizontálně škálovat pomocí sharding.

#### Pravidla nasazení

Při více uzlech graf je nutné řídit nasazení nových podgrafů tak, aby stejný podgraf nebyl indexován dvěma různými uzly, což by vedlo ke kolizím. To lze provést pomocí pravidel nasazení, která mohou také určit, do kterého `shardu` mají být data podgrafu uložena, pokud se používá rozdělení databáze. Pravidla nasazení mohou odpovídat názvu podgrafu a síti, kterou nasazení indexuje, aby bylo možné učinit rozhodnutí.

Příklad konfigurace pravidla nasazení:

```toml
[deployment]
[[deployment.rule]]
match = { name = "(vip|important)/.*" }
shard = "vip"
indexers = [ "index_node_vip_0", "index_node_vip_1" ]
[[deployment.rule]]
match = { network = "kovan" }
# No shard, so we use the default shard called 'primary'
indexers = [ "index_node_kovan_0" ]
[[deployment.rule]]
match = { network = [ "xdai", "poa-core" ] }
indexers = [ "index_node_other_0" ]
[[deployment.rule]]
# There's no 'match', so any subgraph matches
shards = [ "sharda", "shardb" ]
indexers = [
    "index_node_community_0",
    "index_node_community_1",
    "index_node_community_2",
    "index_node_community_3",
    "index_node_community_4",
    "index_node_community_5"
  ]
```

Více informací o pravidlech nasazení [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#controlling-deployment).

#### Vyhrazené dotazovací uzly

Uzly lze nakonfigurovat tak, aby byly explicitně dotazovacími uzly, a to tak, že do konfiguračního souboru vložíte následující údaje:

```toml
[general]
query = "<regular expression>"
```

Každý uzel, jehož --node-id odpovídá regulárnímu výrazu, bude nastaven tak, aby odpovídal pouze na dotazy.

#### Škálování databáze pomocí sharding

Pro většinu případů použití postačuje k podpoře instance graf uzlu jedna databáze Postgres. Pokud instance graf uzlu přeroste rámec jedné databáze Postgres, je možné rozdělit ukládání dat grafového uzlu do více databází Postgres. Všechny databáze dohromady tvoří úložiště instance graf uzlu. Každá jednotlivá databáze se nazývá shard.

Střepy lze použít k rozdělení nasazení dílčích graf do více databází a lze je také použít k použití replik k rozložení zátěže dotazů mezi databázemi. To zahrnuje konfiguraci počtu dostupných databázových připojení, které by měl každý `graf-node` udržovat ve svém fondu připojení pro každou databázi, což je stále důležitější, když se indexuje více podgrafů.

Sharding se stává užitečným, když vaše stávající databáze nedokáže udržet krok se zátěží, kterou na ni Graf Uzel vyvíjí, a když už není možné zvětšit velikost databáze.

> Obecně je lepší vytvořit jednu co největší databázi, než začít s oddíly. Jednou z výjimek jsou případy, kdy je provoz dotazů rozdělen velmi nerovnoměrně mezi dílčí podgrafy; v těchto situacích může výrazně pomoci, pokud jsou dílčí podgrafy s velkým objemem uchovávány v jednom shardu a vše ostatní v jiném, protože toto nastavení zvyšuje pravděpodobnost, že data pro dílčí podgrafu s velkým objemem zůstanou v interní cache db a nebudou nahrazena daty, která nejsou tolik potřebná z dílčích podgrafů s malým objemem.

Pokud jde o konfiguraci připojení, začněte s max_connections v souboru postgresql.conf nastaveným na 400 (nebo možná dokonce 200) a podívejte se na metriky store_connection_wait_time_ms a store_connection_checkout_count Prometheus. Výrazné čekací doby (cokoli nad 5 ms) jsou známkou toho, že je k dispozici příliš málo připojení; vysoké čekací doby tam budou také způsobeny tím, že databáze je velmi vytížená (například vysoké zatížení procesoru). Pokud se však databáze jinak jeví jako stabilní, vysoké čekací doby naznačují potřebu zvýšit počet připojení. V konfiguraci je horní hranicí, kolik připojení může každá instance graf uzlu používat, a graf uzel nebude udržovat otevřená připojení, pokud je nepotřebuje.

Více informací o konfiguraci obchodu [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#configuring-multiple-databases).

#### Vyhrazené zpracování bloků

Pokud je nakonfigurováno více uzlů, je nutné určit jeden uzel, který je zodpovědný za přijímání nových bloků, aby všechny nakonfigurované indexové uzly neprováděly dotazování hlavy řetězce. To se provádí v rámci jmenného prostoru `chains`, kde se zadává `id_uzlu`, který se má používat pro přijímání bloků:

```toml
[chains]
ingestor = "block_ingestor_node"
```

#### Podpora více sítí

Graf protokol zvyšuje počet sítí podporovaných pro indexaci odměn a existuje mnoho podgrafů indexujících nepodporované sítě, které by indexátor rád zpracoval. Soubor `config.toml` umožňuje expresivní a flexibilní konfiguraci:

- Více sítí
- Více poskytovatelů na síť (to může umožnit rozdělení zátěže mezi poskytovatele a také konfiguraci plných uzlů i archivních uzlů, přičemž Graph Node může preferovat levnější poskytovatele, pokud to daná pracovní zátěž umožňuje).
- Další údaje o poskytovateli, jako jsou funkce, ověřování a typ poskytovatele (pro experimentální podporu Firehose)

Sekce `[chains]` řídí, ke kterým poskytovatelům ethereum se graf uzel připojuje a kde jsou uloženy bloky a další metadata pro jednotlivé řetězce. Následující příklad konfiguruje dva řetězce, mainnet a kovan, přičemž bloky pro mainnet jsou uloženy ve shard vip a bloky pro kovan jsou uloženy v primárním shard. Řetězec mainnet může používat dva různé poskytovatele, zatímco kovan má pouze jednoho poskytovatele.

```toml
[chains]
ingestor = "block_ingestor_node"
[chains.mainnet]
shard = "vip"
provider = [
  { label = "mainnet1", url = "http://..", features = [], headers = { Authorization = "Bearer foo" } },
  { label = "mainnet2", url = "http://..", features = [ "archive", "traces" ] }
]
[chains.kovan]
shard = "primary"
provider = [ { label = "kovan", url = "http://..", features = [] } ]
```

Více informací o konfiguraci poskytovatele [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/config.md#configuring-ethereum-providers).

### Proměnná prostředí

Graf Uzel podporuje řadu proměnných prostředí, které mohou povolit funkce nebo změnit chování Graf Uzel. Jsou zdokumentovány [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/environment-variables.md).

### Průběžné nasazování

Uživatelé, kteří provozují škálované nastavení indexování s pokročilou konfigurací, mohou využít správu svých graf uzlů pomocí Kubernetes.

- V úložišti indexeru je [příklad odkazu Kubernetes](https://github.com/graphprotocol/indexer/tree/main/k8s)
- [Launchpad](https://docs.graphops.xyz/launchpad/intro) je sada nástrojů pro provozování Graf Protocol Indexeru v Kubernetes, kterou spravuje společnost GraphOps. Poskytuje sadu grafů Helm a CLI pro správu nasazení uzlu Graf.

### Správa uzlu graf

Vzhledem k běžícímu uzlu Graf (nebo uzlům Graf Uzel!) je pak úkolem spravovat rozmístěné podgrafy v těchto uzlech. Graf Uzel nabízí řadu nástrojů, které pomáhají se správou podgrafů.

#### Protokolování

Protokoly Graf Uzel mohou poskytnout užitečné informace pro ladění a optimalizaci Graf Uzel a konkrétních podgrafů. Graf Uzel podporuje různé úrovně protokolů prostřednictvím proměnné prostředí `GRAPH_LOG` s následujícími úrovněmi: error, warn, info, debug nebo trace.

Kromě toho nastavení `GRAPH_LOG_QUERY_TIMING` na `gql` poskytuje více podrobností o tom, jak dotazy GraphQL probíhají (i když to bude generovat velký objem protokolů).

#### Monitorování & upozornění

Graf Uzel poskytuje metriky prostřednictvím koncového bodu Prometheus na portu 8040 ve výchozím nastavení. K vizualizaci těchto metrik lze pak použít nástroj Grafana.

Úložiště indexeru poskytuje [příklad konfigurace Grafana](https://github.com/graphprotocol/indexer/blob/main/k8s/base/grafana.yaml).

#### Graphman

`graphman` je nástroj pro údržbu Graf Uzel, který pomáhá s diagnostikou a řešením různých každodenních i výjimečných úloh.

Příkaz graphman je součástí oficiálních kontejnerů a můžete jej spustit pomocí docker exec do kontejneru graph-node. Vyžaduje soubor `config.toml`.

Úplná dokumentace příkazů `graphman` je k dispozici v úložišti Graf Uzel. Viz \[/docs/graphman.md\] (https://github.com/graphprotocol/graph-node/blob/master/docs/graphman.md) v Graf Uzel `/docs`

### Práce s podgrafy

#### Stav indexování API

API pro stav indexování, které je ve výchozím nastavení dostupné na portu 8030/graphql, nabízí řadu metod pro kontrolu stavu indexování pro různé podgrafy, kontrolu důkazů indexování, kontrolu vlastností podgrafů a další.

Úplné schéma je k dispozici [zde](https://github.com/graphprotocol/graph-node/blob/master/server/index-node/src/schema.graphql).

#### Výkonnost indexování

Proces indexování má tři samostatné části:

- Získávání zajímavých událostí od zprostředkovatele
- Zpracování událostí v pořadí pomocí příslušných obslužných (to může zahrnovat volání řetězce pro zjištění stavu a načtení dat z úložiště)
- Zápis výsledných dat do úložiště

Tyto fáze jsou spojeny do potrubí (tj. mohou být prováděny paralelně), ale jsou na sobě závislé. Pokud se podgrafy indexují pomalu, bude příčina záviset na konkrétním podgrafu.

Běžné příčiny pomalého indexování:

- Čas potřebný k nalezení relevantních událostí z řetězce (zejména obsluhy volání mohou být pomalé vzhledem k závislosti na `trace_filter`)
- Vytváření velkého počtu `eth_calls` jako součást obslužných
- Velké množství interakcí s úložištěm během provádění
- Velké množství dat k uložení do úložiště
- Velký počet událostí ke zpracování
- Pomalá doba připojení k databázi u přeplněných uzlů
- Samotný poskytovatel se dostává za hlavu řetězu
- Pomalé načítání nových účtenek od poskytovatele v hlavě řetězce

Metriky indexování podgrafů mohou pomoci diagnostikovat hlavní příčinu pomalého indexování. V některých případech spočívá problém v samotném podgrafu, ale v jiných případech mohou zlepšení síťových poskytovatelů, snížení konfliktů v databázi a další zlepšení konfigurace výrazně zlepšit výkon indexování.

#### Neúspěšné podgrafy

Během indexování mohou dílčí graf selhat, pokud narazí na neočekávaná data, pokud některá komponenta nefunguje podle očekávání nebo pokud je chyba ve zpracovatelích událostí nebo v konfiguraci. Existují dva obecné typy selhání:

- Deterministická selhání: jedná se o selhání, která nebudou vyřešena opakovanými pokusy
- Nedeterministická selhání: mohou být způsobena problémy se zprostředkovatelem nebo neočekávanou chybou grafického uzlu. Pokud dojde k nedeterministickému selhání, uzel Graf zopakuje selhání obsluhy a postupně se vrátí zpět.

V některých případech může být chyba řešitelná indexátorem (například pokud je chyba důsledkem toho, že není k dispozici správný typ zprostředkovatele, přidání požadovaného zprostředkovatele umožní pokračovat v indexování). V jiných případech je však nutná změna v kódu podgrafu.

> Deterministická selhání jsou považována za "konečná" a pro selhávající blok je vygenerován důkaz indexace, zatímco nedeterministická selhání nikoli, protože podgraf může selhat a pokračovat v indexaci. V některých případech je nedeterministické označení nesprávné a podgraf chybu nikdy nepřekoná; taková selhání by měla být hlášena jako problémy v úložišti Uzel Graf.

#### Bloková a volací mezipaměť

Uzel Graf ukládá určitá data do mezipaměti v úložišti, aby se ušetřilo opětovné načítání od zprostředkovatele. Bloky jsou ukládány do mezipaměti, stejně jako výsledky `eth_calls` (ty jsou ukládány do mezipaměti jako konkrétní blok). Toto ukládání do mezipaměti může výrazně zvýšit rychlost indexování při "resynchronizaci" mírně pozměněného podgrafu.

Pokud však uzel Etherea po určitou dobu poskytoval nesprávná data, mohou se v některých případech dostat do mezipaměti, což může vést k nesprávným datům nebo neúspěšným podgrafům. V takovém případě mohou indexery použít `graphman` k vymazání otrávené cache a následnému přetočení postižených podgrafů, které pak načtou čerstvá data od (doufejme) zdravého poskytovatele.

Pokud existuje podezření na nekonzistenci blokové mezipaměti, například chybějící událost tx receipt:

1. `graphman chain list` pro zjištění názvu řetězce.
2. `graphman chain check-blocks <CHAIN> by-number <NUMBER>` zkontroluje, zda blok uložený v mezipaměti odpovídá poskytovateli, a pokud ne, odstraní blok z mezipaměti.
   1. Pokud existuje rozdíl, může být bezpečnější zkrátit celou mezipaměť pomocí `graphman chain truncate <CHAIN>`.
   2. Pokud se blok shoduje s poskytovatelem, lze problém ladit přímo proti poskytovateli.

#### Problémy a chyby při dotazování

Jakmile je podgraf indexován, lze očekávat, že indexery budou obsluhovat dotazy prostřednictvím koncového bodu vyhrazeného pro dotazy podgrafu. Pokud indexátor doufá, že bude obsluhovat značný objem dotazů, doporučuje se použít vyhrazený uzel pro dotazy a v případě velmi vysokého objemu dotazů mohou indexátory chtít nakonfigurovat oddíly replik tak, aby dotazy neovlivňovaly proces indexování.

I s vyhrazeným dotazovacím uzlem a replikami však může provádění některých dotazů trvat dlouho a v některých případech může zvýšit využití paměti a negativně ovlivnit dobu dotazování ostatních uživatelů.

Neexistuje jedna "stříbrná kulka", ale celá řada nástrojů pro prevenci, diagnostiku a řešení pomalých dotazů.

##### Ukládání dotazů do mezipaměti

Graf Uzel ve výchozím nastavení ukládá dotazy GraphQL do mezipaměti, což může výrazně snížit zatížení databáze. To lze dále konfigurovat pomocí nastavení `GRAPH_QUERY_CACHE_BLOCKS` a `GRAPH_QUERY_CACHE_MAX_MEM` - více informací [zde](https://github.com/graphprotocol/graph-node/blob/master/docs/environment-variables.md#graphql-caching).

##### Analysing queries

Problematic queries most often surface in one of two ways. In some cases, users themselves report that a given query is slow. In that case the challenge is to diagnose the reason for the slowness - whether it is a general issue, or specific to that subgraph or query. And then of course to resolve it, if possible.

In other cases, the trigger might be high memory usage on a query node, in which case the challenge is first to identify the query causing the issue.

Indexers can use [qlog](https://github.com/graphprotocol/qlog/) to process and summarize Graph Node's query logs. `GRAPH_LOG_QUERY_TIMING` can also be enabled to help identify and debug slow queries.

Given a slow query, indexers have a few options. Of course they can alter their cost model, to significantly increase the cost of sending the problematic query. This may result in a reduction in the frequency of that query. However this often doesn't resolve the root cause of the issue.

##### Account-like optimisation

Database tables that store entities seem to generally come in two varieties: 'transaction-like', where entities, once created, are never updated, i.e., they store something akin to a list of financial transactions, and 'account-like' where entities are updated very often, i.e., they store something like financial accounts that get modified every time a transaction is recorded. Account-like tables are characterized by the fact that they contain a large number of entity versions, but relatively few distinct entities. Often, in such tables the number of distinct entities is 1% of the total number of rows (entity versions)

For account-like tables, `graph-node` can generate queries that take advantage of details of how Postgres ends up storing data with such a high rate of change, namely that all of the versions for recent blocks are in a small subsection of the overall storage for such a table.

The command `graphman stats show <sgdNNNN`> shows, for each entity type/table in a deployment, how many distinct entities, and how many entity versions each table contains. That data is based on Postgres-internal estimates, and is therefore necessarily imprecise, and can be off by an order of magnitude. A `-1` in the `entities` column means that Postgres believes that all rows contain a distinct entity.

In general, tables where the number of distinct entities are less than 1% of the total number of rows/entity versions are good candidates for the account-like optimization. When the output of `graphman stats show` indicates that a table might benefit from this optimization, running `graphman stats show <sgdNNN> <table>` will perform a full count of the table - that can be slow, but gives a precise measure of the ratio of distinct entities to overall entity versions.

Once a table has been determined to be account-like, running `graphman stats account-like <sgdNNN>.<table>` will turn on the account-like optimization for queries against that table. The optimization can be turned off again with `graphman stats account-like --clear <sgdNNN>.<table>` It takes up to 5 minutes for query nodes to notice that the optimization has been turned on or off. After turning the optimization on, it is necessary to verify that the change does not in fact make queries slower for that table. If you have configured Grafana to monitor Postgres, slow queries would show up in `pg_stat_activity`in large numbers, taking several seconds. In that case, the optimization needs to be turned off again.

For Uniswap-like subgraphs, the `pair` and `token` tables are prime candidates for this optimization, and can have a dramatic effect on database load.

#### Removing subgraphs

> This is new functionality, which will be available in Graph Node 0.29.x

At some point an indexer might want to remove a given subgraph. This can be easily done via `graphman drop`, which deletes a deployment and all it's indexed data. The deployment can be specified as either a subgraph name, an IPFS hash `Qm..`, or the database namespace `sgdNNN`. Further documentation is available [here](https://github.com/graphprotocol/graph-node/blob/master/docs/graphman.md#-drop).
